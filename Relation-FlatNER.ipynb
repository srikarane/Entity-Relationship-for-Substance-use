{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1-A9MLXTj-aazLRNTNs7aTYskiNDHoFTT","authorship_tag":"ABX9TyNbyCEMByyNagxzi49bV+iX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b7bca52dfd9745c6b76c5e88dc136244":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fca6b9427672422abc07aefac950f0b3","IPY_MODEL_0b5ecda46268453394547883630f011a","IPY_MODEL_1df24afef0884933b6b564cf39b4f03d"],"layout":"IPY_MODEL_2d35bf1f66284faf867d8f3672035347"}},"fca6b9427672422abc07aefac950f0b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_696008a6976d44d0a5cc4fb21471116c","placeholder":"​","style":"IPY_MODEL_6f5af1d132c74fa39fa05e22f77b055d","value":"Downloading builder script: 100%"}},"0b5ecda46268453394547883630f011a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaeb29309dab4b8f8da05f45197ce8ec","max":6338,"min":0,"orientation":"horizontal","style":"IPY_MODEL_327ddfed75a84399b6b37a3241c68615","value":6338}},"1df24afef0884933b6b564cf39b4f03d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9eba94e831b47c4872d512e20cf331a","placeholder":"​","style":"IPY_MODEL_fdb09d2914754817b513bdd81ab4deb3","value":" 6.34k/6.34k [00:00&lt;00:00, 418kB/s]"}},"2d35bf1f66284faf867d8f3672035347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"696008a6976d44d0a5cc4fb21471116c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f5af1d132c74fa39fa05e22f77b055d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aaeb29309dab4b8f8da05f45197ce8ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"327ddfed75a84399b6b37a3241c68615":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9eba94e831b47c4872d512e20cf331a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb09d2914754817b513bdd81ab4deb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9d79ccf8fd74305bb503954588a282c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c648cbb59778434bb065b93e328b6c59","IPY_MODEL_54871d635026456d9e324263efcbc8b1","IPY_MODEL_afe825a6f445453988b58f65d4e05bc6"],"layout":"IPY_MODEL_73f06936392f4139bfbd17a7a3c2c954"}},"c648cbb59778434bb065b93e328b6c59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f10a0df0cd6475a9ecec0f6552c2b07","placeholder":"​","style":"IPY_MODEL_1aa6250fe4244a9db5c12b6e1e202ffc","value":"vocab.txt: 100%"}},"54871d635026456d9e324263efcbc8b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce5e5362b5c647e2b4a20d5017df4aeb","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec58d854ea07433892bdf2782d3cbcb5","value":213450}},"afe825a6f445453988b58f65d4e05bc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e459739dd3640c3a044f0d3cb9d1609","placeholder":"​","style":"IPY_MODEL_5305ed35ccc54d299cfbe89d09dbb5f1","value":" 213k/213k [00:00&lt;00:00, 448kB/s]"}},"73f06936392f4139bfbd17a7a3c2c954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f10a0df0cd6475a9ecec0f6552c2b07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aa6250fe4244a9db5c12b6e1e202ffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce5e5362b5c647e2b4a20d5017df4aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec58d854ea07433892bdf2782d3cbcb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e459739dd3640c3a044f0d3cb9d1609":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5305ed35ccc54d299cfbe89d09dbb5f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e196b68d51f4a6184aefbf049299853":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b624fc32e024cadb3f2bceef45dae8b","IPY_MODEL_3c46775151bf48c39e5b31bf80760461","IPY_MODEL_f508fb8e2ea8496d84c37d8912befcb3"],"layout":"IPY_MODEL_9ac89cf2baab4ee58689750337b69f6d"}},"1b624fc32e024cadb3f2bceef45dae8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1031bea245b4135a905b2b5bec7dea5","placeholder":"​","style":"IPY_MODEL_5bb99619531a415a80e34d8afc019648","value":"config.json: 100%"}},"3c46775151bf48c39e5b31bf80760461":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f57fd589af14864b810375611ba0489","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68e5f7fa47794b6aac7127875a2690bf","value":385}},"f508fb8e2ea8496d84c37d8912befcb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b970a556a104fc7b0fc1858bc60c8fa","placeholder":"​","style":"IPY_MODEL_c0693e5d783b48b48cb14aff653fbb98","value":" 385/385 [00:00&lt;00:00, 32.7kB/s]"}},"9ac89cf2baab4ee58689750337b69f6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1031bea245b4135a905b2b5bec7dea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb99619531a415a80e34d8afc019648":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f57fd589af14864b810375611ba0489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68e5f7fa47794b6aac7127875a2690bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b970a556a104fc7b0fc1858bc60c8fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0693e5d783b48b48cb14aff653fbb98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bed2bada541044358f4cf1175ac1c837":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e2b35b2697542a29c294e09ce174cd6","IPY_MODEL_10b4919e35e347478d165ccd04e00679","IPY_MODEL_f4788c5208ec44489a33eb40a70ace52"],"layout":"IPY_MODEL_fc1894c4747040248b08e45ac84e3b52"}},"3e2b35b2697542a29c294e09ce174cd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95396c62138f4135bab0a37c6e7b875d","placeholder":"​","style":"IPY_MODEL_b9dcc85a85434bf6841c6a4562ac3cfd","value":"pytorch_model.bin: 100%"}},"10b4919e35e347478d165ccd04e00679":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d2826169d2a4565ac7998709cb27ef1","max":435778770,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c9209a06c0e42409939f6e15b7f3518","value":435778770}},"f4788c5208ec44489a33eb40a70ace52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd3f73b078c747819a39eb477083b996","placeholder":"​","style":"IPY_MODEL_08a43a3cca414e39b0b4f8217fba5752","value":" 436M/436M [00:25&lt;00:00, 15.7MB/s]"}},"fc1894c4747040248b08e45ac84e3b52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95396c62138f4135bab0a37c6e7b875d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9dcc85a85434bf6841c6a4562ac3cfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d2826169d2a4565ac7998709cb27ef1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c9209a06c0e42409939f6e15b7f3518":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd3f73b078c747819a39eb477083b996":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08a43a3cca414e39b0b4f8217fba5752":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Entity Relation model (parallel task learning):\n","*   Performing named entity recognition for entities and roles at token - level and forming relation pairs and predicting relations.\n","* Evaluating Relations and ner models.\n","\n","*   Entities and Role which are not considered due to insufficient data for training:\n","  1.  discarded_enities=['EnvironmentalExposure','SexualHistory','InfectiousDiseases','PhysicalActivity','Residence','LivingSituation','MaritalStatus','Occupation']\n","\n","  2.   discarded_roles=['LivingStatus','Other','MedicalCondition','Extent','History']\n","\n","*   Labelling is done using BIO-scheme.\n","*   Labels are generated using the mapping between label offsets and Bert token offsets.\n","*   In entities and roles, observations indicate that certain documents exhibit overlapping spans among them, as well as between roles and entities.\n","*   To handle overlapping between roles and entities we are classifying it with seperate heads.\n","\n","*   By analysis, it was observed by seperating status, method from the roles we can avoid overlapping among them, so even here classifying with seperate heads.\n","\n","*   Overlapping between the Entities where between 'Family','Residence','LivingSituation','MaritalStatus'. Except 'Family' all other entities have been dropped.\n","*    Generated relation mapping with token indices and labels for each event present in a document.\n","### **Experimentations:**\n","*  By eliminating overlapping entities, does the performance of the model in recognizing family entities improve?\n","*   Does the model improves by training entities (discarding overlapping), roles and relations together?\n","\n","\n","  ###   **Model**\n","*   ***Tokenizer:*** BertTokenizerFast\n","*   ***pre-trained Bert model:*** 'emilyalsentzer/Bio_ClinicalBERT'\n","\n","*   ***Hyperparameters:***\n","  * eps=1e-8\n","  * learning_rate=7e-5\n","  * weight_decay=0\n","  * num_train_epochs=15\n","  * patience=3\n","  * batch_size=16\n","  * max_len_token=512\n","*   Fine-tuned a pre-trained Bert model with  multiple classification heads where each classifies status, methods, entities, and  roles respectively and also relations using BERT embeddings and cross-span attention. Cross span attention is bi-directional to capture complex relations.\n","*   Since we are training multiple tasks simultaneously, we aggregate the losses and send them for backpropagation.\n","\n","*   Sequence evaluation is used as NER metrics.\n","\n","*   Model stored at savemodel path + model_type+ver.pth.\n"," ****\n","### **Run the notebook::**\n","* Provide model_type and ver and project directory in input section and run all cells.\n"," datasets and their paths:\n","*   train_data_set path: project_directory+'/data/trainset.json'\n","\n","*   test_data_set path: project_directory+'/data/testset.json'\n","* Model stored at project_directory+'/models/'+model_name+'_'+ver+'.pth'.\n","* Evaluation dataset predictions are stored at 'tst_'+model_type+'_'+str(ver)+'.json\n","### **Metrics:**"],"metadata":{"id":"keRVoVKYSs3k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rkCSxQd2I9T"},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import evaluate\n","from transformers import BertTokenizerFast, BertModel, AdamW, get_linear_schedule_with_warmup, DataCollatorForTokenClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm, trange\n","import itertools"],"metadata":{"id":"axyMLJVSDPGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"id":"HUTmznXeDRD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric=evaluate.load(\"seqeval\")\n","device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = BertTokenizerFast.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","num_freeze_layers=6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235,"referenced_widgets":["b7bca52dfd9745c6b76c5e88dc136244","fca6b9427672422abc07aefac950f0b3","0b5ecda46268453394547883630f011a","1df24afef0884933b6b564cf39b4f03d","2d35bf1f66284faf867d8f3672035347","696008a6976d44d0a5cc4fb21471116c","6f5af1d132c74fa39fa05e22f77b055d","aaeb29309dab4b8f8da05f45197ce8ec","327ddfed75a84399b6b37a3241c68615","e9eba94e831b47c4872d512e20cf331a","fdb09d2914754817b513bdd81ab4deb3","c9d79ccf8fd74305bb503954588a282c","c648cbb59778434bb065b93e328b6c59","54871d635026456d9e324263efcbc8b1","afe825a6f445453988b58f65d4e05bc6","73f06936392f4139bfbd17a7a3c2c954","7f10a0df0cd6475a9ecec0f6552c2b07","1aa6250fe4244a9db5c12b6e1e202ffc","ce5e5362b5c647e2b4a20d5017df4aeb","ec58d854ea07433892bdf2782d3cbcb5","9e459739dd3640c3a044f0d3cb9d1609","5305ed35ccc54d299cfbe89d09dbb5f1","9e196b68d51f4a6184aefbf049299853","1b624fc32e024cadb3f2bceef45dae8b","3c46775151bf48c39e5b31bf80760461","f508fb8e2ea8496d84c37d8912befcb3","9ac89cf2baab4ee58689750337b69f6d","e1031bea245b4135a905b2b5bec7dea5","5bb99619531a415a80e34d8afc019648","6f57fd589af14864b810375611ba0489","68e5f7fa47794b6aac7127875a2690bf","7b970a556a104fc7b0fc1858bc60c8fa","c0693e5d783b48b48cb14aff653fbb98"]},"id":"xIgALejlDVBw","executionInfo":{"status":"ok","timestamp":1706743864298,"user_tz":300,"elapsed":6379,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"626152df-741f-4576-a7a1-ab68d68fa3b1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7bca52dfd9745c6b76c5e88dc136244"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d79ccf8fd74305bb503954588a282c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e196b68d51f4a6184aefbf049299853"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Datasets"],"metadata":{"id":"IyidLGrbBHlz"}},{"cell_type":"markdown","source":["### Parameter initialization and Dataset path"],"metadata":{"id":"cKtbQBkMA_GJ"}},{"cell_type":"code","source":["model_type='Relation_Ner_parallel'\n","ver=2\n","\n","discarded_enities=['EnvironmentalExposure','SexualHistory','InfectiousDiseases','PhysicalActivity','Residence','LivingSituation','MaritalStatus','Occupation']\n","discarded_roles=['LivingStatus','Other','MedicalCondition','Extent','History']\n","\n","\n","project_directory='/content/drive/MyDrive/PHD_assessment_gmu/'\n","raw_dataset_path=project_directory+'data/'+'SocialHistoryMTSamples.json'\n","train_dataset_path=project_directory+'data/'+'trainset.json'\n","test_dataset_path=project_directory+'data/'+'testset.json'\n","save_model_path=project_directory+'/models/'\n","bert_model_name='emilyalsentzer/Bio_ClinicalBERT'\n","\n","eps=1e-8\n","learning_rate=7e-5\n","weight_decay=0.01\n","num_train_epochs=15\n","patience=5\n","max_len=512\n","\n","batch_size=16"],"metadata":{"id":"2vh8n4J4DYSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id_label_status={0:'O',1:'B-Status',2:'I-Status'}\n","id_label_method={0:'O',1:'B-Method',2:'I-Method'}\n","id_label_role={0:'O',1:'B-Type',2:'I-Type',3:'B-Amount',4:'I-Amount',5:'B-Temporal',6:'I-Temporal',7:'B-Frequency',8:'I-Frequency',9:'B-QuitHistory',10:'I-QuitHistory',11:'B-ExposureHistory',12:'I-ExposureHistory',13:'B-Location',14:'I-Location'}\n","id_label_event={0:'No Relation',1:'Relation'}\n","label_id_status = {v: k for k, v in id_label_status.items()}\n","label_id_method = {v: k for k, v in id_label_method.items()}\n","label_id_role = {v: k for k, v in id_label_role.items()}\n","label_id_ent = {'B-Alcohol':1,\n"," 'B-Drug':3,\n"," 'B-Family':5,\n"," 'B-Tobacco':7,\n"," 'I-Alcohol':2,\n"," 'I-Drug':4,\n"," 'I-Family':6,\n"," 'I-Tobacco':8,\n"," 'O':0}\n","\n","id_label_ent = {v: k for k, v in label_id_ent.items()}\n","label_id_event = {v: k for k, v in id_label_event.items()}\n"],"metadata":{"id":"C1dII3zPDzUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z-9CnLbBglis"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate Relation label and pairs and entity, role labels\n"],"metadata":{"id":"RiYKlag5BSA7"}},{"cell_type":"code","source":["\n","class GenerateLabel:\n","\n","  @staticmethod\n","  def generate_enity_labels(entity_list, token_len, token_offsets):\n","    '''\n","    Generates entity labels for each token in the sentence. Performs by mapping the entity's offset positions to the tokens offset.\n","\n","    Inputs:\n","    entity_list: List of entity dicts, each entity dict has the information such as offset positions, id , category and text.\n","    token_len: Initializes the labels size\n","    token_offsets: list of tuples of offset mappings of bert tokens.\n","    file_name: Name of the file from which the entity list is generated.\n","\n","    Outputs:\n","    entity_labels: List of BIO labels for each token.\n","    '''\n","    # Initialize a list to store the BIO labels for each token\n","\n","    entity_labels = [label_id_ent['O']] * token_len\n","    for entity in entity_list:\n","        category = entity['entity_category']\n","        if category not in discarded_enities:\n","          entity_start_pos = int(entity['entity_strt_pos'])\n","          entity_end_pos = int(entity['entity_end_pos'])-1\n","\n","          # Find tokens that correspond to the entity's position\n","          entity_start_token = None\n","          entity_end_token = None\n","\n","          for i, (start_offset, end_offset) in enumerate(token_offsets):\n","              if entity_start_token is None and start_offset >= entity_start_pos:\n","                  entity_start_token = i\n","              if end_offset > entity_end_pos:\n","                  entity_end_token = i\n","                  break\n","          if entity_start_token is not None:\n","            entity_labels[entity_start_token] = label_id_ent['B-' + category]\n","            if entity_end_token is not None:\n","                entity_labels[entity_start_token + 1:entity_end_token + 1] = [label_id_ent['I-' + category]] * (entity_end_token - entity_start_token)\n","\n","\n","\n","    return entity_labels\n","\n","  @staticmethod\n","  def generate_role_labels(role_list, token_len, token_offsets):\n","    '''\n","    Generates role labels for each token in the sentence. Performs by mapping the entity's offset positions to the tokens offset.\n","    Also handles overlapping entities by seperating status and method from the role and creating seperate labels for it.\n","\n","    Inputs:\n","    role_list: List of role dicts, each entity dict has the information such as offset positions, id , category and text.\n","    token_len: Initializes the labels size\n","    token_offsets: list of tuples of offset mappings of bert tokens.\n","    file_name: Name of the file from which the entity list is generated.\n","\n","    Outputs:\n","    role_labels: List of BIO labels for each token.\n","    status_labels: List of BIO labels for each token.\n","    method_labels: List of BIO labels for each token.\n","    '''\n","    # Initialize a list to store the BIO labels for each token\n","    role_labels = [label_id_role['O']] * token_len\n","    status_labels = [label_id_status['O']] * token_len\n","    method_labels = [label_id_method['O']] * token_len\n","\n","    for role in role_list:\n","        category = role['entity_category']\n","        entity_start_pos = int(role['entity_strt_pos'])\n","        entity_end_pos = int(role['entity_end_pos'])-1\n","        if category in discarded_roles:\n","          continue\n","        # Find tokens that correspond to the entity's position\n","        entity_start_token = None\n","        entity_end_token = None\n","\n","        for i, (start_offset, end_offset) in enumerate(token_offsets):\n","            if entity_start_token is None and start_offset >= entity_start_pos:\n","                entity_start_token = i\n","            if end_offset > entity_end_pos:\n","                entity_end_token = i\n","                break\n","\n","        # Assign BIO labels to the tokens\n","        if category == 'Status':\n","          if entity_start_token is not None:\n","            status_labels[entity_start_token] = label_id_status['B-' + category]\n","            if entity_end_token is not None:\n","                status_labels[entity_start_token + 1:entity_end_token + 1] = [label_id_status['I-' + category]] * (entity_end_token - entity_start_token)\n","\n","        elif category == 'Method':\n","          if entity_start_token is not None:\n","            method_labels[entity_start_token] = label_id_method['B-' + category]\n","            if entity_end_token is not None:\n","                method_labels[entity_start_token + 1:entity_end_token + 1] = [label_id_method['I-' + category]] * (entity_end_token - entity_start_token)\n","\n","        else:\n","          if entity_start_token is not None:\n","            role_labels[entity_start_token] = label_id_role['B-' + category]\n","            if entity_end_token is not None:\n","                role_labels[entity_start_token + 1:entity_end_token + 1] = [label_id_role['I-' + category]] * (entity_end_token - entity_start_token)\n","\n","\n","    return role_labels, status_labels, method_labels\n","\n","  @staticmethod\n","  def generate_relation_labels( token_offsets, data):\n","     '''\n","    Generates relation mapping with token indices for each event present in a document.\n","    Reads events and maps them to their related entities and roles with their token indices.\n","\n","\n","    Inputs:\n","    token_offsets: list of tuples of offset mappings of bert tokens.\n","    data: Data dict which consists of entity_list, role_list and events_list.\n","\n","    Outputs:\n","    relation_labels: The relation mapping is a dictionary where key is a tuple of entity token indices and value is a list of tuple of role token indices.\n","\n","    '''\n","    entity_token_indices = {}\n","    role_token_indices = {}\n","    event_categories = {}\n","\n","    entity_list = data.get('entity_list', [])\n","    role_list = data.get('role_list', [])\n","    events_list = data.get('events_list', [])\n","\n","    # Create a dictionary to map entity IDs to their token indices\n","    for entity in entity_list:\n","        entity_id = entity['entity_id']\n","        entity_start_pos = int(entity['entity_strt_pos'])\n","        entity_end_pos = int(entity['entity_end_pos'])-1\n","        entity_category = entity['entity_category']\n","\n","        entity_start_token = None\n","        entity_end_token = None\n","\n","        for i, (start_offset, end_offset) in enumerate(token_offsets):\n","            if entity_start_token is None and start_offset >= entity_start_pos:\n","                entity_start_token = i\n","            if end_offset > entity_end_pos:\n","                entity_end_token = i\n","                break\n","\n","        if entity_start_token is not None:\n","            entity_token_indices[entity_id] = (entity_start_token, entity_end_token, entity_category)\n","\n","    # Create a dictionary to map event-related role IDs to their token indices\n","    for event in events_list:\n","        entity_id = event['entity_id']\n","        related_roles = event['Related_roles']\n","        ent_info=entity_token_indices.get(entity_id, None)\n","\n","\n","        if ent_info:\n","            en_strt,en_end,ent_cat=ent_info\n","            entity_indices = (en_strt,en_end)\n","            role_indices = []\n","            role_categories = []\n","\n","            for role_id in related_roles:\n","                role = next((role for role in role_list if role['role_id'] == role_id), None)\n","\n","                if role:\n","                    role_start_pos = int(role['entity_strt_pos'])\n","                    role_end_pos = int(role['entity_end_pos'])-1\n","\n","                    role_start_token = None\n","                    role_end_token = None\n","\n","                    for i, (start_offset, end_offset) in enumerate(token_offsets):\n","                        if role_start_token is None and start_offset >= role_start_pos:\n","                            role_start_token = i\n","                        if end_offset > role_end_pos:\n","                            role_end_token = i\n","                            break\n","\n","                    if role_start_token is not None:\n","                        role_indices.append((role_start_token, role_end_token))\n","                        role_categories.append(((role_start_token, role_end_token),role['entity_category']))\n","\n","            if role_indices:\n","                role_token_indices[(entity_indices[0],entity_indices[1])] = role_indices\n","                event_categories[((entity_indices[0],entity_indices[1]),ent_cat)] = role_categories\n","\n","    return role_token_indices, event_categories\n","\n","  @staticmethod\n","  def generate_relation_pairs(relation_labels):\n","    '''\n","    Generates relation pairs from relation mapping.\n","\n","    Inputs:\n","    ;relation_labels: The relation mapping is a dictionary where key is a tuple of entity token indices and value is a list of tuple of role token indices.\n","    ;return: related_pairs, related_logits, related_pairs_categories\n","    related_pairs: list of token indices pairs of entity and role/status/method.\n","    related_logits: list of relation label for each pair.\n","    related_pairs_categories: list of categories for each pair.\n","    '''\n","    related_pairs = []\n","    not_related_pairs = []\n","    related_pairs_categories = []\n","    not_related_pairs_categories = []\n","\n","    # Extract all unique value pairs\n","    all_value_pairs = set()\n","    for value_list in relation_labels.values():\n","        for value in value_list:\n","            all_value_pairs.add(value)\n","\n","    # Iterate through each key-value pair in the dictionary\n","    for key, values in relation_labels.items():\n","        value_set = set(values)\n","\n","        # Add related pairs\n","        for value in value_set:\n","            related_pairs.append((key[0], value[0]))\n","            related_pairs_categories.append((key[1], value[1]))\n","\n","        # Add not related pairs\n","        not_related_value_pairs = all_value_pairs - value_set\n","        for value in not_related_value_pairs:\n","            not_related_pairs.append((key[0], value[0]))\n","            not_related_pairs_categories.append((key[1], value[1]))\n","\n","    related_logits = [1]*len(related_pairs)\n","    not_related_logits = [0]*len(not_related_pairs)\n","    related_pairs = related_pairs + not_related_pairs\n","    related_logits = related_logits + not_related_logits\n","    related_pairs_categories = related_pairs_categories + not_related_pairs_categories\n","    return related_pairs, related_logits, related_pairs_categories"],"metadata":{"id":"qKuSBY9_D5bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset creation and loading"],"metadata":{"id":"F2rHBQ7TBl4K"}},{"cell_type":"code","source":["class ERDataset(Dataset):\n","  def __init__(self, data, tokenizer, max_len):\n","    self.data = data\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  def __len__(self):\n","    return len(self.data)\n","  def __getitem__(self, idx):\n","    item=self.data[idx]\n","    text = item['text']\n","    inputs = self.tokenizer(text,max_length=max_len,truncation=True,return_offsets_mapping=True)\n","    tokens_len=len(inputs['input_ids'])\n","    offset_mapping_list=inputs['offset_mapping']\n","    entity_labels=GenerateLabel.generate_enity_labels(item['entity_list'],tokens_len,offset_mapping_list)\n","    role_labels, status_labels, method_labels=GenerateLabel.generate_role_labels(item['role_list'],tokens_len,offset_mapping_list)\n","    relation_labels,event_categories_relation=GenerateLabel.generate_relation_labels(offset_mapping_list,item)\n","    related_pairs, related_logits, related_pairs_categories=GenerateLabel.generate_relation_pairs(event_categories_relation)\n","\n","    return {\n","      'input_ids':inputs['input_ids'],\n","      'attention_mask':inputs['attention_mask'],\n","      'entity_labels':entity_labels,\n","      'role_labels':role_labels,\n","      'status_labels':status_labels,\n","      'method_labels':method_labels,\n","      'relation_labels':event_categories_relation,\n","      'text':text,\n","      'file_name':item['file_name'],\n","      'offset_mapping':offset_mapping_list,\n","      'tokens':inputs.tokens(),\n","      'relation_pairs':related_pairs,\n","      'related_logits':related_logits,\n","      'related_pairs_categories':related_pairs_categories\n","    }\n","def pad_span_positions(span_positions):\n","  '''\n","  Pads span positions to the same length.\n","  '''\n","  max_length = max(len(spans) for spans in span_positions)\n","  padded_spans = []\n","  span_masks = []\n","  for spans in span_positions:\n","    if spans==[]:\n","      padded = [((-1, -1),(-1, -1))] * max_length\n","    else:\n","      padded = spans + [((-1, -1),(-1, -1))] * (max_length - len(spans))\n","    padded_spans.append(padded)\n","  return torch.tensor(padded_spans)\n","def collate_fn_entity_role(batch):\n","  input_ids = [torch.tensor(x['input_ids']) for x in batch]\n","  attention_mask = [torch.tensor(x['attention_mask']) for x in batch]\n","  #token_type_ids = [torch.tensor(x['token_type_ids']) for x in batch]\n","  tokens=[x['tokens'] for x in batch]\n","  file_name=[x['file_name'] for x in batch]\n","  relation_labels=[x['relation_labels'] for x in batch]\n","  relation_pairs_btch=[x['relation_pairs'] for x in batch]\n","  related_logits=[x['related_logits'] for x in batch]\n","  text=[x['text'] for x in batch]\n","  related_pairs_categories=[x['related_pairs_categories'] for x in batch]\n","  '''\n","  for x in batch:\n","    for idx in range(0,len(x['entity_labels'])):\n","      if isinstance(x['entity_labels'][idx],list):\n","        mutiple_labels=x['entity_labels'][idx]\n","        print(mutiple_labels)\n","        sing_mul_label=''\n","        for lab in mutiple_labels:\n","          if sing_mul_label=='':\n","            sing_mul_label=str(lab)\n","          else:\n","            sing_mul_label=sing_mul_label+'900'+str(lab)\n","        x['entity_labels'][idx] =int(sing_mul_label)\n","        print(x['entity_labels'][idx])\n","  '''\n","  entity_labels = [torch.tensor(x['entity_labels']) for x in batch]\n","  role_labels = [torch.tensor(x['role_labels']) for x in batch]\n","  status_labels = [torch.tensor(x['status_labels']) for x in batch]\n","  method_labels = [torch.tensor(x['method_labels']) for x in batch]\n","  input_ids = pad_sequence(input_ids, batch_first=True,padding_value=tokenizer.pad_token_id)\n","  attention_mask = pad_sequence(attention_mask, batch_first=True,padding_value=0)\n","  #token_type_ids = pad_sequence(token_type_ids, batch_first=True,padding_value=0)\n","  entity_labels = pad_sequence(entity_labels, batch_first=True,padding_value=-100)\n","  role_labels = pad_sequence(role_labels, batch_first=True,padding_value=-100)\n","  status_labels = pad_sequence(status_labels, batch_first=True,padding_value=-100)\n","  method_labels = pad_sequence(method_labels, batch_first=True,padding_value=-100)\n","  relation_pairs = pad_span_positions(relation_pairs_btch)\n","  return {\n","    'input_ids':input_ids,\n","    'attention_mask':attention_mask,\n","    'entity_labels':entity_labels,\n","    'role_labels':role_labels,\n","    'status_labels':status_labels,\n","    'method_labels':method_labels,\n","    'tokens':tokens,\n","    'file_name':file_name,\n","    'relation_labels':relation_labels,\n","    'relation_pairs':relation_pairs,\n","    'related_logits':related_logits,\n","    'text':text,\n","    'related_pairs_categories':related_pairs_categories,\n","    'relation_pairs_btch':relation_pairs_btch\n","  }\n"],"metadata":{"id":"CyYiZHqfBpyi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(train_dataset_path, 'r', encoding='utf-8') as file:\n","  train_data=json.load(file)\n","with open(test_dataset_path, 'r', encoding='utf-8') as file:\n","  test_data=json.load(file)\n","train_dataset=ERDataset(train_data,tokenizer, max_len)\n","test_dataset=ERDataset(test_data,tokenizer, max_len)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn_entity_role)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_entity_role)"],"metadata":{"id":"eikrM1phCCGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"L8kRTtBIBdlC"}},{"cell_type":"code","source":["class CrossSpanAttention(nn.Module):\n","    '''\n","    Cross Span Attention Layer:\n","    This layer performs an attention mechanism where one set of spans (span2) attends to another set of spans (span1).\n","    It is designed to capture inter-span relationships and contextual dependencies within a sequence.\n","\n","    '''\n","    def __init__(self, embed_size):\n","        super(CrossSpanAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.keys = nn.Linear(self.embed_size, self.embed_size)\n","        self.queries = nn.Linear(self.embed_size, self.embed_size)\n","        self.values = nn.Linear(self.embed_size, self.embed_size)\n","\n","    def forward(self, span1_embeddings, span2_embeddings, span1_mask, span2_mask):\n","        '''\n","        Computes the attention-weighted output.\n","        - span1_embeddings: Embeddings for span1 tokens.\n","        - span2_embeddings: Embeddings for span2 tokens.\n","        - span1_mask: Mask for span1 to ignore padding tokens during attention computation.\n","        - span2_mask: Mask for span2 to apply attention weights only to valid tokens.\n","\n","        The forward pass computes keys from span1 embeddings, queries, and values from span2 embeddings, applies a scaled dot-product attention mechanism, and returns the attention-weighted average of span2 values.\n","\n","\n","        '''\n","        span1_keys = self.keys(span1_embeddings)\n","        span2_queries = self.queries(span2_embeddings)\n","        span2_values = self.values(span2_embeddings)\n","\n","        attention = torch.matmul(span2_queries, span1_keys.transpose(-2, -1)) / self.embed_size ** 0.5\n","        attention = attention.masked_fill(span1_mask.unsqueeze(1), -1e9)\n","        attention = torch.softmax(attention, dim=-1)\n","\n","        assert not torch.isnan(attention).any(), \"NaNs found in attention after softmax\"\n","        span2_values = span2_values * span2_mask.unsqueeze(-1)\n","        out = torch.matmul(attention, span2_values)\n","\n","        out = out.mean(dim=0,keepdim=True)\n","        return out\n","\n","class EntityRelationClassifier(nn.Module):\n","    '''\n","    module for classifying entity relationships using BERT embeddings and cross-span attention.\n","\n","    '''\n","    def __init__(self, model_name, num_freeze_layers,num_status_labels,num_method_labels,num_role_labels,num_entity_labels,num_relation_classes, dropout=0.1):\n","        super().__init__()\n","        self.bertmodel = BertModel.from_pretrained(model_name)\n","        self.cross_span_attention = CrossSpanAttention(self.bertmodel.config.hidden_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.status_classifier = nn.Linear(self.bertmodel.config.hidden_size, num_status_labels)\n","        self.method_classifier = nn.Linear(self.bertmodel.config.hidden_size, num_method_labels)\n","        self.role_classifier = nn.Linear(self.bertmodel.config.hidden_size, num_role_labels)\n","        self.entity_classifier = nn.Linear(self.bertmodel.config.hidden_size, num_entity_labels)\n","        self.relation_classifier = nn.Linear(self.bertmodel.config.hidden_size * 2, num_relation_classes)\n","\n","    def forward(self, input_ids, attention_mask, all_span_positions):\n","        '''\n","        Args:\n","        input_ids (torch.Tensor): Indices of input sequence tokens in the vocabulary.\n","        attention_mask (torch.Tensor): Mask to avoid attention on padding token indices.\n","        all_span_positions (List of Lists): Span positions for all potential entities and roles in each example of the batch.\n","\n","        Returns:\n","        Tuple containing logits for status, method, role, entity predictions, and a list of all relation predictions,\n","        and their corresponding logits.\n","        '''\n","\n","        # Pass inputs through BERT model and apply dropout to the sequence output\n","        bert_output = self.bertmodel(input_ids=input_ids, attention_mask=attention_mask)\n","        sequence_output= bert_output[0]\n","        bert_output = self.dropout(bert_output[0])\n","\n","        # Generate logits for each classification task using the respective linear layers\n","        status_logits = self.status_classifier(bert_output)\n","        method_logits = self.method_classifier(bert_output)\n","        role_logits = self.role_classifier(bert_output)\n","        entity_logits = self.entity_classifier(bert_output)\n","\n","        # Initialize lists to store the logits and predictions for relationships\n","        all_relation_logits = []\n","        all_relation_preds = []\n","\n","        # Process each example in the batch\n","        batch_size = input_ids.size(0)\n","        for i in range(batch_size):\n","\n","            entity_pairs = all_span_positions[i]\n","            batch_relation_logits = []\n","\n","            # Process each entity pair\n","\n","            for span1, span2 in entity_pairs:\n","                # Skip padded spans\n","                if torch.equal(span1,torch.tensor([-1, -1]).to(device)) or torch.equal(span2,torch.tensor([-1, -1]).to(device)):\n","                    continue\n","                # Determine the maximum length for padding based on the spans\n","                max_length=max((span1[1]+1) - span1[0], (span2[1]+1) - span2[0])\n","                # Extract embeddings and padding for each span\n","                span1_embeddings = sequence_output[i, span1[0]:span1[1]+1, :]\n","                span1_embeddings, span1_mask = self.pad_and_create_mask(span1_embeddings, max_length)\n","                span2_embeddings = sequence_output[i, span2[0]:span2[1]+1, :]\n","                span2_embeddings, span2_mask = self.pad_and_create_mask(span2_embeddings, max_length)\n","\n","                # Apply bi-directional cross-span attention for capturing the complex relationships and contextual nuances between text segments\n","                span1_to_span2_attention = self.cross_span_attention(span1_embeddings, span2_embeddings, span1_mask, span2_mask)\n","                span2_to_span1_attention = self.cross_span_attention(span2_embeddings, span1_embeddings, span2_mask, span1_mask)\n","\n","\n","\n","                # Concatenate and classify the relation\n","                combined_embedding = torch.cat((span1_to_span2_attention, span2_to_span1_attention), dim=1)\n","                #predict relations\n","                relation_logits = self.relation_classifier(combined_embedding)\n","\n","                all_relation_logits.append(relation_logits)\n","\n","                probabilities = nn.functional.softmax(relation_logits, dim=1)\n","\n","                labels = torch.argmax(probabilities, dim=1)\n","\n","                batch_relation_logits.append(labels.item())\n","            all_relation_preds.append(batch_relation_logits)\n","        # Combine logits from all entities and relations\n","        all_relation_logits = torch.cat(all_relation_logits, dim=0)\n","        return status_logits,method_logits,role_logits,entity_logits, all_relation_preds,all_relation_logits\n","    def pad_and_create_mask(self, embeddings, max_length):\n","      '''\n","      Pad the embeddings to the maximum length\n","      '''\n","      pad_size = max_length - embeddings.size(0)\n","      if pad_size > 0:\n","          pad = torch.zeros((pad_size, embeddings.size(1)), device=embeddings.device)\n","          mask = torch.cat([torch.ones(embeddings.size(0), device=embeddings.device), torch.zeros(pad_size, device=embeddings.device)])\n","          embeddings = torch.cat([embeddings, pad], dim=0)\n","      else:\n","          mask = torch.ones(embeddings.size(0), device=embeddings.device)\n","\n","      # Convert the mask to boolean\n","      mask = mask.bool()\n","\n","      return embeddings, mask\n","\n","\n"],"metadata":{"id":"DShlzkv26ThA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Evaluation"],"metadata":{"id":"NyITmqsKCMOK"}},{"cell_type":"code","source":["def compute_ner_metric(preds,labels,id_label):\n","    ground_truths=[[id_label[l] for l in lab if l!=-100] for lab in labels]\n","    prediction_labels=[[id_label[p] for p,l in zip(predict,lab) if l!=-100] for predict,lab in zip(preds,labels)]\n","    metric_res=metric.compute(predictions=prediction_labels,references=ground_truths)\n","    return metric_res\n","def compute_relation_metric(preds,labels,id_label):\n","    metric_res=classification_report(labels, preds, target_names=[id_label[0],id_label[1]])\n","    return metric_res"],"metadata":{"id":"WpWdjihGIht6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_relation_labels_preds(related_pairs_categories, relation_pairs, related_logits,text,relation_preds,tokens,id_label_event):\n","  doc_relations=[]\n","  for idx in range(0,len(related_pairs_categories)):\n","    entity_token_ind=relation_pairs[idx][0]\n","    role_token_ind=relation_pairs[idx][1]\n","    doc_relations.append({'text':text,'entity':related_pairs_categories[idx][0],\n","    'role':related_pairs_categories[idx][1],\n","\n","    'entity_tokens':tokens[entity_token_ind[0]:entity_token_ind[1]+1],\n","    'role_tokens':tokens[role_token_ind[0]:role_token_ind[1]+1],\n","    'ground_truth':id_label_event[related_logits[idx]],\n","    'prediction':id_label_event[related_logits[idx]]})\n","\n","  return doc_relations"],"metadata":{"id":"yVUW9CG7KIDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_entity_model(model,val_loader,loss_fn,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event,extract_predictions=False,dt_set=''):\n","  '''\n","  Performs evaluation for a given dataset and returns the computed loss and metrics for Entity, Role, Status, Method and relations.\n","  Saves Token level predictions of entity, role, method, status and relations in data/tr_{runname and version}.json\n","  '''\n","  model.eval()\n","  with torch.no_grad():\n","    #Initializing Validation losses for each of categories\n","    val_ent_loss=0\n","    val_status_loss=0\n","    val_method_loss=0\n","    val_role_loss=0\n","    val_average_loss=0\n","    val_event_loss=0\n","\n","    #Initializing labels and prediction list, in-turn used for evaluating metrics.\n","    all_entity_labels=[]\n","    all_entity_predictions=[]\n","    all_role_labels=[]\n","    all_role_predictions=[]\n","    all_method_labels=[]\n","    all_method_predictions=[]\n","    all_status_labels=[]\n","    all_status_predictions=[]\n","    all_event_labels=[]\n","    all_event_predictions=[]\n","\n","    all_res_entity={}\n","    all_res_role={}\n","    all_res_method={}\n","    all_res_status={}\n","    all_res_event=[]\n","    all_res={}\n","    for step,batch in enumerate(val_loader):\n","      #Extracting inputs from the batch.\n","      inputs={'input_ids':batch['input_ids'].to(device),'attention_mask':batch['attention_mask'].to(device),'all_span_positions':batch['relation_pairs'].to(device)}\n","      entity_labels=batch['entity_labels'].to(device)\n","      role_labels=batch['role_labels'].to(device)\n","      method_labels=batch['method_labels'].to(device)\n","      status_labels=batch['status_labels'].to(device)\n","      relation_labels=batch['related_logits']\n","      relation_labels_tensor=[torch.tensor(x) for x in batch['related_logits']]\n","      relation_labels_tensor=torch.cat(relation_labels_tensor,dim=0)\n","      tokens=batch['tokens']\n","      entity_role_cat_rel=batch['related_pairs_categories']\n","\n","      #Forward Pass\n","      status_logits,method_logits,role_logits,entity_logits, relation_preds,relation_logits=model(**inputs)\n","\n","      #Computing Loss\n","      relation_loss=loss_fn(relation_logits.view(-1,len(id_label_event)),relation_labels_tensor.view(-1).to(device))\n","      entity_loss=loss_fn(entity_logits.view(-1,len(id_label_ent)),entity_labels.view(-1))\n","      role_loss=loss_fn(role_logits.view(-1,len(id_label_role)),role_labels.view(-1))\n","      status_loss=loss_fn(status_logits.view(-1,len(id_label_status)),status_labels.view(-1))\n","      method_loss=loss_fn(method_logits.view(-1,len(id_label_method)),method_labels.view(-1))\n","      avg_loss_step=(entity_loss+role_loss+status_loss+method_loss)/4\n","      val_ent_loss+=entity_loss\n","      val_role_loss += role_loss\n","      val_method_loss += method_loss\n","      val_status_loss += status_loss\n","      val_event_loss += relation_loss\n","      val_average_loss += avg_loss_step\n","      #Calculating probabilities and predictions\n","\n","      entity_probabilities = torch.softmax(entity_logits, dim=-1)\n","      entity_predictions = torch.argmax(entity_probabilities, dim=-1)\n","      role_probabilities  = torch.softmax(role_logits,dim=-1)\n","      role_predictions = torch.argmax(role_probabilities, dim =-1)\n","      status_probabilities=torch.softmax(status_logits,dim=-1)\n","      status_predictions=torch.argmax(status_probabilities,dim=-1)\n","\n","      method_probabilities=torch.softmax(method_logits,dim=-1)\n","      method_predictions=torch.argmax(method_probabilities,dim=-1)\n","      all_status_predictions.extend(status_predictions.tolist())\n","      all_status_labels.extend(batch['status_labels'].tolist())\n","      all_method_labels.extend(batch['method_labels'].tolist())\n","      all_method_predictions.extend(method_predictions.tolist())\n","      all_event_labels.extend(batch['related_logits'])\n","      all_event_predictions.extend(relation_preds)\n","\n","      # Mapping Token,Prediction,label and saving to a json file\n","      for idx in range(0,len(batch['file_name'])):\n","        document_relations=process_relation_labels_preds(batch['related_pairs_categories'][idx], batch['relation_pairs_btch'][idx], batch['related_logits'][idx],batch['text'][idx],relation_preds[idx],batch['tokens'][idx],id_label_event)\n","        all_res_event.extend(document_relations)\n","\n","      status_labels=batch['status_labels'].tolist()\n","      status_predictions=status_predictions.tolist()\n","      for idx in range(0,len(batch['file_name'])):\n","        all_res_status[batch['file_name'][idx]]=list(zip(batch['tokens'][idx],status_labels[idx],status_predictions[idx]))\n","\n","\n","      method_labels=batch['method_labels'].tolist()\n","      method_predictions=method_predictions.tolist()\n","      for idx in range(0,len(batch['file_name'])):\n","        all_res_method[batch['file_name'][idx]]=list(zip(batch['tokens'][idx],method_labels[idx],method_predictions[idx]))\n","\n","\n","      all_role_labels.extend(batch['role_labels'].tolist())\n","      all_role_predictions.extend(role_predictions.tolist())\n","      role_labels=batch['role_labels'].tolist()\n","      role_predictions=role_predictions.tolist()\n","      for idx in range(0,len(batch['file_name'])):\n","        all_res_role[batch['file_name'][idx]]=list(zip(batch['tokens'][idx],role_labels[idx],role_predictions[idx]))\n","\n","      all_entity_labels.extend(entity_labels.tolist())\n","      all_entity_predictions.extend(entity_predictions.tolist())\n","      entity_predictions=entity_predictions.tolist()\n","      entity_labels=entity_labels.tolist()\n","      #updated_entity_labels=updated_entity_labels.tolist()\n","      for idx in range(0,len(batch['file_name'])):\n","        all_res_entity[batch['file_name'][idx]]=list(zip(batch['tokens'][idx],entity_labels[idx],entity_predictions[idx]))\n","    #print(all_res_event)\n","    all_res={'entity':all_res_entity,'role':all_res_role,'status':all_res_status,'method':all_res_method,'relations':all_res_event}\n","\n","    #Computing NER metrics\n","    entity_metrics=compute_ner_metric(all_entity_predictions,all_entity_labels,id_label_ent)\n","    role_metrics=compute_ner_metric(all_role_predictions,all_role_labels,id_label_role)\n","    status_metrics=compute_ner_metric(all_status_predictions,all_status_labels,id_label_status)\n","    method_metrics=compute_ner_metric(all_method_predictions,all_method_labels,id_label_method)\n","    all_event_predictions = list(itertools.chain.from_iterable(all_event_predictions))\n","    all_event_labels = list(itertools.chain.from_iterable(all_event_labels))\n","    relation_metrics=compute_relation_metric(all_event_predictions,all_event_labels,id_label_event)\n","    #Saving Results\n","    if extract_predictions:\n","      with open(project_directory+'data/'+dt_set+'_'+model_type+'_'+str(ver)+'.json','w') as f:\n","        json.dump(all_res,f)\n","  return {'avg_loss':val_average_loss/len(val_loader),'entity_evaluation':(val_ent_loss/len(val_loader),entity_metrics),'role_evaluation':(val_role_loss/len(val_loader),role_metrics),\n","            'method_evaluation':(val_method_loss/len(val_loader),method_metrics),'status_evaluation':(val_status_loss/len(val_loader),status_metrics),'event_evaluation':(val_event_loss/len(val_loader),relation_metrics)}\n","\n","\n"],"metadata":{"id":"aPSdy2grqM4w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Training"],"metadata":{"id":"WcdekYDvCPum"}},{"cell_type":"code","source":["def train_entity_bert_model(model,tr_dataloader,tst_dataloader,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event,bert_model_name,training_args):\n","  '''\n","  Performs training with early stopping.\n","  Average loss has been calculated for the multiple tasks and sent for backward propogation.\n","\n","  '''\n","  if torch.cuda.is_available():\n","    model.cuda()\n","  train_cycles=trange(training_args['num_train_epochs'],desc='Epoch',disable=0)\n","  loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n","  #Intializing optimizers and schedulers\n","  optimizer=training_args['optimizer']\n","  scheduler=training_args['scheduler']\n","  patience=training_args['patience']\n","\n","  #Intializing training and validation loss lists for accumulating around the epochs\n","  tr_ent_loss_lst=[]\n","  tr_status_loss_lst=[]\n","  tr_method_loss_lst=[]\n","  tr_role_loss_lst=[]\n","  tr_event_loss_lst=[]\n","  tr_avg_loss_lst=[]\n","\n","  val_ent_loss_list=[]\n","  val_status_loss_list=[]\n","  val_method_loss_list=[]\n","  val_role_loss_list=[]\n","  val_avg_loss_list=[]\n","  val_event_loss_list=[]\n","\n","  early_stopping_count=0\n","  early_stopping_count_ent=0\n","  early_stopping_count_status=0\n","  early_stopping_count_method=0\n","  early_stopping_count_role=0\n","\n","  for cycle in train_cycles:\n","    epoch_cycles=tqdm(tr_dataloader,desc='Iteration',disable=-1)\n","    model.train()\n","    #Intializing epoch loss\n","    tr_ent_loss=0\n","    tr_status_loss=0\n","    tr_method_loss=0\n","    tr_role_loss=0\n","    tr_event_loss=0\n","    average_loss=0\n","\n","    for step,batch in enumerate(epoch_cycles):\n","      optimizer.zero_grad()\n","      inputs={'input_ids':batch['input_ids'].to(device),'attention_mask':batch['attention_mask'].to(device),'all_span_positions':batch['relation_pairs'].to(device)}\n","      entity_labels=batch['entity_labels'].to(device)\n","      #Model Computation\n","      status_logits,method_logits,role_logits,entity_logits, relation_preds, relation_logits=model(**inputs)\n","      role_labels=batch['role_labels'].to(device)\n","      method_labels=batch['method_labels'].to(device)\n","      status_labels=batch['status_labels'].to(device)\n","      relation_labels=batch['related_logits']\n","      relation_labels_tensor=[torch.tensor(x) for x in batch['related_logits']]\n","      relation_labels_tensor=torch.cat(relation_labels_tensor,dim=0)\n","\n","      #Loss computation\n","      relation_loss=loss_fn(relation_logits.view(-1,len(id_label_event)),relation_labels_tensor.view(-1).to(device))\n","      entity_loss=loss_fn(entity_logits.view(-1,len(id_label_ent)),entity_labels.view(-1))\n","      role_loss=loss_fn(role_logits.view(-1,len(id_label_role)),role_labels.view(-1))\n","      status_loss=loss_fn(status_logits.view(-1,len(id_label_status)),status_labels.view(-1))\n","      method_loss=loss_fn(method_logits.view(-1,len(id_label_method)),method_labels.view(-1))\n","      avg_loss_step=(entity_loss+role_loss+status_loss+method_loss+relation_loss)/5\n","\n","      avg_loss_step.backward()\n","      optimizer.step()\n","      scheduler.step()\n","\n","      #Loss computation across an epoch\n","      tr_ent_loss+=entity_loss.item()\n","      tr_status_loss+=status_loss.item()\n","      tr_method_loss+=method_loss.item()\n","      tr_role_loss+=role_loss.item()\n","      tr_event_loss+=relation_loss.item()\n","      average_loss+=avg_loss_step.item()\n","\n","    #Accumulation losses for all epochs.\n","    tr_ent_loss_lst.append(tr_ent_loss/len(tr_dataloader))\n","    tr_status_loss_lst.append(tr_status_loss/len(tr_dataloader))\n","    tr_method_loss_lst.append(tr_method_loss/len(tr_dataloader))\n","    tr_role_loss_lst.append(tr_role_loss/len(tr_dataloader))\n","    tr_event_loss_lst.append(tr_event_loss/len(tr_dataloader))\n","    tr_avg_loss_lst.append(average_loss/len(tr_dataloader))\n","\n","    print('Epoch: {}  Train Entity Loss: {}'.format(cycle,tr_ent_loss_lst[-1]))\n","    print('Epoch: {}  Train Status Loss: {}'.format(cycle,tr_status_loss_lst[-1]))\n","    print('Epoch: {}  Train Method Loss: {}'.format(cycle,tr_method_loss_lst[-1]))\n","    print('Epoch: {}  Train Role Loss:   {}'.format(cycle,tr_role_loss_lst[-1]))\n","    print('Epoch: {}  Train Relation loss:  {}'.format(cycle,tr_event_loss_lst[-1]))\n","    print('Epoch: {}  Train Average Loss: {}'.format(cycle,tr_avg_loss_lst[-1]))\n","\n","    #Computing metrics and evaluating models for validataion data.\n","    #Saves Results on last epoch or when it's reaches patience\n","    if cycle == training_args['num_train_epochs']-1 or early_stopping_count_ent >= patience-1 or early_stopping_count_status >= patience-1  or early_stopping_count_method >= patience-1 or early_stopping_count_role >= patience-1 :\n","      val_results=evaluate_entity_model(model,tst_dataloader,loss_fn,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event,True,'tst')\n","    else:\n","      val_results=evaluate_entity_model(model,tst_dataloader,loss_fn,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event)\n","\n","    val_ent_loss_list.append(val_results['entity_evaluation'][0])\n","    val_status_loss_list.append(val_results['status_evaluation'][0])\n","    val_method_loss_list.append(val_results['method_evaluation'][0])\n","    val_role_loss_list.append(val_results['role_evaluation'][0])\n","    val_event_loss_list.append(val_results['event_evaluation'][0])\n","    val_avg_loss_list.append(val_results['avg_loss'])\n","\n","    print('Epoch: {}  Val Entity Loss: {}'.format(cycle,val_ent_loss_list[-1]))\n","    print('Epoch: {}  Val Status Loss: {}'.format(cycle,val_status_loss_list[-1]))\n","    print('Epoch: {}  Val Method Loss: {}'.format(cycle,val_method_loss_list[-1]))\n","    print('Epoch: {}  Val Role Loss: {}'.format(cycle,val_role_loss_list[-1]))\n","    print('Epoch: {}  Val Event Loss: {}'.format(cycle,val_event_loss_list[-1]))\n","    print('Epoch: {}  Avg Val Loss: {}'.format(cycle, val_avg_loss_list[-1]))\n","    print('Epoch: {}  Metrics below: \\n'.format(cycle))\n","\n","    #Computing metrics and evaluating models for training data.\n","    if cycle == training_args['num_train_epochs']-1 or early_stopping_count_ent >= patience-1 or early_stopping_count_status >= patience-1  or early_stopping_count_method >= patience-1 or early_stopping_count_role >= patience-1 :\n","      tr_results=evaluate_entity_model(model,tr_dataloader,loss_fn,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event,True,'tr')\n","    else:\n","      tr_results=evaluate_entity_model(model,tr_dataloader,loss_fn,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event)\n","\n","\n","    print('Entity_metrics: \\n')\n","    print(val_results['entity_evaluation'][1])\n","    print('\\n')\n","    print(tr_results['entity_evaluation'][1])\n","    print('\\n')\n","    print('Role_metrics: \\n')\n","    print(val_results['role_evaluation'][1])\n","    print('\\n')\n","    print(tr_results['role_evaluation'][1])\n","    print('\\n')\n","    print('Status_metrics: \\n')\n","    print(val_results['status_evaluation'][1])\n","    print('\\n')\n","    print(tr_results['status_evaluation'][1])\n","    print('\\n')\n","    print('Method_metrics: \\n')\n","    print(val_results['method_evaluation'][1])\n","    print('\\n')\n","    print(tr_results['method_evaluation'][1])\n","    print('\\n')\n","    print('Relation_metrics: \\n')\n","    print(val_results['event_evaluation'][1])\n","    print('\\n')\n","    print(tr_results['event_evaluation'][1])\n","    print('\\n')\n","\n","     #Early stopping is implemented by checking for a continuous increase in validation loss over a specified number of epochs.\n","    if cycle==0:\n","      min_ent_val_loss=val_ent_loss_list[-1]\n","      min_status_val_loss=val_status_loss_list[-1]\n","      min_method_val_loss=val_method_loss_list[-1]\n","      min_role_val_loss=val_role_loss_list[-1]\n","      min_val_loss=val_results['avg_loss']\n","      early_stopping_count_ent=0\n","      early_stopping_count_status=0\n","      early_stopping_count_method=0\n","      early_stopping_count_role=0\n","    else:\n","      if val_results['avg_loss']<min_val_loss:\n","        min_val_loss=val_results['avg_loss']\n","        early_stopping_count=0\n","      else:\n","        early_stopping_count+=1\n","      if val_ent_loss_list[-1]<min_ent_val_loss:\n","        min_ent_val_loss=val_ent_loss_list[-1]\n","        early_stopping_count_ent=0\n","      else:\n","        early_stopping_count_ent+=1\n","      if val_status_loss_list[-1]<min_status_val_loss:\n","        min_status_val_loss=val_status_loss_list[-1]\n","        early_stopping_count_status=0\n","      else:\n","        early_stopping_count_status+=1\n","      if val_method_loss_list[-1]<min_method_val_loss:\n","        min_method_val_loss=val_method_loss_list[-1]\n","        early_stopping_count_method=0\n","      else:\n","        early_stopping_count_method+=1\n","      if val_role_loss_list[-1]<min_role_val_loss:\n","        min_role_val_loss=val_role_loss_list[-1]\n","        early_stopping_count_role=0\n","      else:\n","        early_stopping_count_role+=1\n","\n","      if early_stopping_count>=patience or early_stopping_count_ent>=patience or early_stopping_count_status>=patience or early_stopping_count_method>=patience or early_stopping_count_role>=patience:\n","        print('Early stopping counter for entity model : {}'.format(early_stopping_count_ent))\n","        print('Early stopping counter for status model : {}'.format(early_stopping_count_status))\n","        print('Early stopping counter for method model : {}'.format(early_stopping_count_method))\n","        print('Early stopping counter for role model : {}'.format(early_stopping_count_role))\n","        print('Early stopping counter for overall model : {}'.format(early_stopping_count))\n","        print('Early stopping at epoch: {}'.format(cycle))\n","        break\n","\n","  #Model save:\n","  torch.save(model.state_dict(),training_args['output_dir']+training_args['run_name']+'.pth')\n","  return model, {'train_loss':{'tr_ent_loss_lst':tr_ent_loss_lst,'tr_event_loss_lst':tr_event_loss_lst,\n","                 'tr_method_loss_lst':tr_method_loss_lst,'tr_status_loss_lst':tr_status_loss_lst,'tr_role_loss_lst':tr_role_loss_lst,'tr_avg_loss_lst':tr_avg_loss_lst},\n","                 'val_loss':{'val_event_loss_list':val_event_loss_list,'val_ent_loss_list':val_ent_loss_list,'val_status_loss_list':val_status_loss_list,'val_method_loss_list':val_method_loss_list,'val_avg_loss_list':val_avg_loss_list}}\n","\n","\n","\n","\n","\n"],"metadata":{"id":"YRhGlSCLqo2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model= EntityRelationClassifier(model_name=bert_model_name,num_freeze_layers=num_freeze_layers,num_status_labels=len(id_label_status),num_method_labels=len(id_label_method),num_role_labels=len(id_label_role),num_entity_labels=len(id_label_ent),num_relation_classes=len(id_label_event))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["bed2bada541044358f4cf1175ac1c837","3e2b35b2697542a29c294e09ce174cd6","10b4919e35e347478d165ccd04e00679","f4788c5208ec44489a33eb40a70ace52","fc1894c4747040248b08e45ac84e3b52","95396c62138f4135bab0a37c6e7b875d","b9dcc85a85434bf6841c6a4562ac3cfd","2d2826169d2a4565ac7998709cb27ef1","7c9209a06c0e42409939f6e15b7f3518","fd3f73b078c747819a39eb477083b996","08a43a3cca414e39b0b4f8217fba5752"]},"id":"kH7AnrMv5rPk","executionInfo":{"status":"ok","timestamp":1706743896026,"user_tz":300,"elapsed":29545,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"9f8cdef4-6b10-4343-93e5-c2825762b019"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed2bada541044358f4cf1175ac1c837"}},"metadata":{}}]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=learning_rate,eps=eps)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*num_train_epochs)\n","training_args={\n","    'output_dir':save_model_path,\n","    'num_train_epochs':num_train_epochs,\n","    'optimizer':optimizer,\n","    'scheduler':scheduler,\n","    'patience':patience,\n","    'run_name':model_type+'_'+str(ver)\n","\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpHn5wAIAeE_","executionInfo":{"status":"ok","timestamp":1706743896358,"user_tz":300,"elapsed":344,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"9ea905d0-50ff-4b26-be79-27f0a5187008"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["trained_model,loss=train_entity_bert_model(model,train_dataloader,test_dataloader,id_label_ent,id_label_role,id_label_status,id_label_method,id_label_event,bert_model_name,training_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JPLr4KNGylp8","executionInfo":{"status":"ok","timestamp":1706745749019,"user_tz":300,"elapsed":400246,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"88e1bb9f-7397-4f85-f6f8-4f8772e617bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/15 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0  Train Entity Loss: 0.07764063424923841\n","Epoch: 0  Train Status Loss: 0.07206000541062917\n","Epoch: 0  Train Method Loss: 0.02911323495209217\n","Epoch: 0  Train Role Loss:   0.10538096572546397\n","Epoch: 0  Train Relation loss:  0.4622990962337045\n","Epoch: 0  Train Average Loss: 0.14929878843181274\n","Epoch: 0  Val Entity Loss: 0.175241157412529\n","Epoch: 0  Val Status Loss: 0.1376015692949295\n","Epoch: 0  Val Method Loss: 0.06696680933237076\n","Epoch: 0  Val Role Loss: 0.31744521856307983\n","Epoch: 0  Val Event Loss: 0.5046505331993103\n","Epoch: 0  Avg Val Loss: 0.17431369423866272\n","Epoch: 0  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:   7%|▋         | 1/15 [01:06<15:27, 66.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Epoch: 1  Train Entity Loss: 0.07787775445510359\n","Epoch: 1  Train Status Loss: 0.07197036265450366\n","Epoch: 1  Train Method Loss: 0.030048548956127727\n","Epoch: 1  Train Role Loss:   0.10216581010643173\n","Epoch: 1  Train Relation loss:  0.4634903213557075\n","Epoch: 1  Train Average Loss: 0.14911056090803707\n","Epoch: 1  Val Entity Loss: 0.175241157412529\n","Epoch: 1  Val Status Loss: 0.1376015692949295\n","Epoch: 1  Val Method Loss: 0.06696680933237076\n","Epoch: 1  Val Role Loss: 0.31744521856307983\n","Epoch: 1  Val Event Loss: 0.5046505331993103\n","Epoch: 1  Avg Val Loss: 0.17431369423866272\n","Epoch: 1  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  13%|█▎        | 2/15 [02:13<14:25, 66.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Epoch: 2  Train Entity Loss: 0.07883555464008275\n","Epoch: 2  Train Status Loss: 0.07229384210179834\n","Epoch: 2  Train Method Loss: 0.030516990186537012\n","Epoch: 2  Train Role Loss:   0.10426290574319222\n","Epoch: 2  Train Relation loss:  0.46268218580414266\n","Epoch: 2  Train Average Loss: 0.14971829994636424\n","Epoch: 2  Val Entity Loss: 0.175241157412529\n","Epoch: 2  Val Status Loss: 0.1376015692949295\n","Epoch: 2  Val Method Loss: 0.06696680933237076\n","Epoch: 2  Val Role Loss: 0.31744521856307983\n","Epoch: 2  Val Event Loss: 0.5046505331993103\n","Epoch: 2  Avg Val Loss: 0.17431369423866272\n","Epoch: 2  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  20%|██        | 3/15 [03:19<13:16, 66.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Epoch: 3  Train Entity Loss: 0.07828222083694794\n","Epoch: 3  Train Status Loss: 0.07169146791977041\n","Epoch: 3  Train Method Loss: 0.029156998785979608\n","Epoch: 3  Train Role Loss:   0.10311117373845156\n","Epoch: 3  Train Relation loss:  0.4628574742990382\n","Epoch: 3  Train Average Loss: 0.14901987112620296\n","Epoch: 3  Val Entity Loss: 0.175241157412529\n","Epoch: 3  Val Status Loss: 0.1376015692949295\n","Epoch: 3  Val Method Loss: 0.06696680933237076\n","Epoch: 3  Val Role Loss: 0.31744521856307983\n","Epoch: 3  Val Event Loss: 0.5046505331993103\n","Epoch: 3  Avg Val Loss: 0.17431369423866272\n","Epoch: 3  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  27%|██▋       | 4/15 [04:25<12:08, 66.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Epoch: 4  Train Entity Loss: 0.07832941566320027\n","Epoch: 4  Train Status Loss: 0.07146148778059903\n","Epoch: 4  Train Method Loss: 0.030297898632638594\n","Epoch: 4  Train Role Loss:   0.10281614564797457\n","Epoch: 4  Train Relation loss:  0.4637248445959652\n","Epoch: 4  Train Average Loss: 0.14932595806963303\n","Epoch: 4  Val Entity Loss: 0.175241157412529\n","Epoch: 4  Val Status Loss: 0.1376015692949295\n","Epoch: 4  Val Method Loss: 0.06696680933237076\n","Epoch: 4  Val Role Loss: 0.31744521856307983\n","Epoch: 4  Val Event Loss: 0.5046505331993103\n","Epoch: 4  Avg Val Loss: 0.17431369423866272\n","Epoch: 4  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  33%|███▎      | 5/15 [05:32<11:05, 66.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Epoch: 5  Train Entity Loss: 0.0786392682177179\n","Epoch: 5  Train Status Loss: 0.07134446414077983\n","Epoch: 5  Train Method Loss: 0.030003098661408704\n","Epoch: 5  Train Role Loss:   0.10285890277694254\n","Epoch: 5  Train Relation loss:  0.46283390416818504\n","Epoch: 5  Train Average Loss: 0.14913592750535293\n","Epoch: 5  Val Entity Loss: 0.175241157412529\n","Epoch: 5  Val Status Loss: 0.1376015692949295\n","Epoch: 5  Val Method Loss: 0.06696680933237076\n","Epoch: 5  Val Role Loss: 0.31744521856307983\n","Epoch: 5  Val Event Loss: 0.5046505331993103\n","Epoch: 5  Avg Val Loss: 0.17431369423866272\n","Epoch: 5  Metrics below: \n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  33%|███▎      | 5/15 [06:39<13:18, 79.86s/it]"]},{"output_type":"stream","name":"stdout","text":["Entity_metrics: \n","\n","{'Alcohol': {'precision': 0.8148148148148148, 'recall': 0.8, 'f1': 0.8073394495412846, 'number': 55}, 'Drug': {'precision': 0.9032258064516129, 'recall': 0.9333333333333333, 'f1': 0.9180327868852459, 'number': 30}, 'Family': {'precision': 0.5416666666666666, 'recall': 0.7027027027027027, 'f1': 0.611764705882353, 'number': 37}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.7671957671957672, 'overall_recall': 0.8285714285714286, 'overall_f1': 0.7967032967032968, 'overall_accuracy': 0.9784445091756481}\n","\n","\n","{'Alcohol': {'precision': 0.8398058252427184, 'recall': 0.8693467336683417, 'f1': 0.8543209876543211, 'number': 199}, 'Drug': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1': 0.8571428571428571, 'number': 124}, 'Family': {'precision': 0.83125, 'recall': 0.910958904109589, 'f1': 0.8692810457516339, 'number': 146}, 'Tobacco': {'precision': 0.9026548672566371, 'recall': 0.9066666666666666, 'f1': 0.9046563192904656, 'number': 225}, 'overall_precision': 0.8583333333333333, 'overall_recall': 0.8904899135446686, 'overall_f1': 0.8741159830268741, 'overall_accuracy': 0.9870735498650086}\n","\n","\n","Role_metrics: \n","\n","{'Amount': {'precision': 0.7931034482758621, 'recall': 0.8363636363636363, 'f1': 0.8141592920353982, 'number': 55}, 'ExposureHistory': {'precision': 0.5, 'recall': 0.6, 'f1': 0.5454545454545454, 'number': 10}, 'Frequency': {'precision': 0.8148148148148148, 'recall': 0.7857142857142857, 'f1': 0.7999999999999999, 'number': 28}, 'Location': {'precision': 0.4074074074074074, 'recall': 0.6875, 'f1': 0.5116279069767441, 'number': 16}, 'QuitHistory': {'precision': 0.6428571428571429, 'recall': 0.6, 'f1': 0.6206896551724138, 'number': 15}, 'Temporal': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1': 0.56, 'number': 11}, 'Type': {'precision': 0.6558441558441559, 'recall': 0.7593984962406015, 'f1': 0.7038327526132404, 'number': 133}, 'overall_precision': 0.6601307189542484, 'overall_recall': 0.753731343283582, 'overall_f1': 0.7038327526132404, 'overall_accuracy': 0.9327119137780367}\n","\n","\n","{'Amount': {'precision': 0.8577586206896551, 'recall': 0.9299065420560748, 'f1': 0.8923766816143499, 'number': 214}, 'ExposureHistory': {'precision': 0.5348837209302325, 'recall': 0.6388888888888888, 'f1': 0.5822784810126581, 'number': 36}, 'Frequency': {'precision': 0.8558558558558559, 'recall': 0.8558558558558559, 'f1': 0.8558558558558559, 'number': 111}, 'Location': {'precision': 0.90625, 'recall': 0.90625, 'f1': 0.90625, 'number': 64}, 'QuitHistory': {'precision': 0.6875, 'recall': 0.7333333333333333, 'f1': 0.7096774193548386, 'number': 30}, 'Temporal': {'precision': 0.775, 'recall': 0.8378378378378378, 'f1': 0.8051948051948051, 'number': 37}, 'Type': {'precision': 0.9162011173184358, 'recall': 0.9479768786127167, 'f1': 0.9318181818181819, 'number': 519}, 'overall_precision': 0.8687440982058546, 'overall_recall': 0.9099901088031652, 'overall_f1': 0.8888888888888891, 'overall_accuracy': 0.9837192178679539}\n","\n","\n","Status_metrics: \n","\n","{'Status': {'precision': 0.6195121951219512, 'recall': 0.654639175257732, 'f1': 0.6365914786967419, 'number': 194}, 'overall_precision': 0.6195121951219512, 'overall_recall': 0.654639175257732, 'overall_f1': 0.6365914786967419, 'overall_accuracy': 0.9551412758520245}\n","\n","\n","{'Status': {'precision': 0.8425806451612903, 'recall': 0.8547120418848168, 'f1': 0.8486029889538661, 'number': 764}, 'overall_precision': 0.8425806451612903, 'overall_recall': 0.8547120418848168, 'overall_f1': 0.8486029889538661, 'overall_accuracy': 0.9828192751370367}\n","\n","\n","Method_metrics: \n","\n","{'Method': {'precision': 0.4444444444444444, 'recall': 0.6060606060606061, 'f1': 0.5128205128205128, 'number': 33}, 'overall_precision': 0.4444444444444444, 'overall_recall': 0.6060606060606061, 'overall_f1': 0.5128205128205128, 'overall_accuracy': 0.9796096708418293}\n","\n","\n","{'Method': {'precision': 0.7956204379562044, 'recall': 0.8134328358208955, 'f1': 0.8044280442804429, 'number': 134}, 'overall_precision': 0.7956204379562044, 'overall_recall': 0.8134328358208955, 'overall_f1': 0.8044280442804429, 'overall_accuracy': 0.9954184733698764}\n","\n","\n","Relation_metrics: \n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.80      0.97      0.87      1906\n","    Relation       0.53      0.13      0.21       539\n","\n","    accuracy                           0.78      2445\n","   macro avg       0.66      0.55      0.54      2445\n","weighted avg       0.74      0.78      0.73      2445\n","\n","\n","\n","              precision    recall  f1-score   support\n","\n"," No Relation       0.82      0.97      0.89      8100\n","    Relation       0.66      0.19      0.29      2116\n","\n","    accuracy                           0.81     10216\n","   macro avg       0.74      0.58      0.59     10216\n","weighted avg       0.79      0.81      0.77     10216\n","\n","\n","\n","Early stopping counter for entity model : 5\n","Early stopping counter for status model : 5\n","Early stopping counter for method model : 5\n","Early stopping counter for role model : 5\n","Early stopping counter for overall model : 5\n","Early stopping at epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}