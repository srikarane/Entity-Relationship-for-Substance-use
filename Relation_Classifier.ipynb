{"cells":[{"cell_type":"markdown","source":["### Relation classifier:\n","Performing relation classification gives an entity and a role.\n","\n","\n","\n","*   Text is embedded with special tokens ['[SPAN1_START]', '[SPAN1_END]', '[SPAN2_START]', '[SPAN2_END]'] and added to the tokenizer.\n","*   These special tokens are embedded before and after span positions of entities and role. Span1 is for entities and Span2 is for Roles.\n","* Labels are generated for relation pairs present in relation labels of the dataset. Labels are mapped to relation or no relation on basis of the relation mapping.\n","* Utilized BertForSequenceClassification for model training.\n","* Adjusted tokenizer and model to accomodate special tokens.\n","###   **Model**\n","*   ***Tokenizer:*** BertTokenizerFast\n","*   ***pre-trained Bert model:*** 'emilyalsentzer/Bio_ClinicalBERT'\n","### ***Hyperparameters:***\n","* batch_size=16\n","* eps=1e-8\n","* learning_rate=5e-5\n","* weight_decay=0\n","* num_train_epochs=10\n","* patience=3\n","\n","*   Model stored at project_directory+'models/final_models/Indepent_relation_classifier_v'+str(ver)+'/'\n","### **Inputs:**\n","Datasets and their paths:\n","*   train_data_set path: project_directory+'/data/trainset.json'\n","\n","\n","*   test_data_set path: project_directory+'/data/testset.json'\n","### **Metrics:**\n","\n","           precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99      1938\n","           1       0.96      0.94      0.95       539\n","\n","    accuracy                           0.98      2477\n","   macro avg       0.97      0.97      0.97      2477\n","weighted avg       0.98      0.98      0.98      2477\n"],"metadata":{"id":"2Va73jqkbmhl"}},{"cell_type":"code","source":[],"metadata":{"id":"lXPtbuQgZ7dh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Loading and hyperparameters Intialization"],"metadata":{"id":"ufEVpDJDaMa3"}},{"cell_type":"markdown","source":["### Parameters Initialization"],"metadata":{"id":"aKs1kg0ubg2y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsg_ctbw80h4"},"outputs":[],"source":["import pandas as pd\n","from transformers import BertTokenizerFast, BertModel, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm, trange\n","import json"]},{"cell_type":"code","source":["project_directory='/content/drive/MyDrive/PHD_assessment_gmu/'\n","save_model_path=project_directory+'/models/'"],"metadata":{"id":"ZY7iHJ0ee8_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31rubPk59Nmx"},"outputs":[],"source":["ver=6\n","device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = BertTokenizerFast.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')\n","batch_size=16\n","eps=1e-8\n","learning_rate=5e-5\n","weight_decay=0\n","num_train_epochs=10\n","patience=3\n","discarded_enities=['EnvironmentalExposure','SexualHistory','InfectiousDiseases','PhysicalActivity']\n","discarded_roles=['LivingStatus','Other','MedicalCondition','Extent','History']\n","max_len=512\n","min_label_size=10\n","\n","raw_dataset_path=project_directory+'data/'+'SocialHistoryMTSamples.json'\n","train_dataset_path=project_directory+'data/'+'trainset.json'\n","test_dataset_path=project_directory+'data/'+'testset.json'\n","bert_model_name='emilyalsentzer/Bio_ClinicalBERT'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRAB414d9ffz"},"outputs":[],"source":["id_label_status={0:'O',1:'B-Status',2:'I-Status'}\n","id_label_method={0:'O',1:'B-Method',2:'I-Method'}\n","id_label_role={0:'O',1:'B-Type',2:'I-Type',3:'B-Amount',4:'I-Amount',5:'B-Temporal',6:'I-Temporal',7:'B-Frequency',8:'I-Frequency',9:'B-QuitHistory',10:'I-QuitHistory',11:'B-ExposureHistory',12:'I-ExposureHistory',13:'B-Location',14:'I-Location'}\n","id_label_ent={0:'O',1:'B-Tobacco',2:'I-Tobacco',3:'B-Alcohol',4:'I-Alcohol',5:'B-Family',6:'I-Family',7:'B-Drug',8:'I-Drug',9:'B-Occupation',10:'I-Occupation',11:'B-MaritalStatus',12:'I-MaritalStatus',13:'B-LivingSituation',14:'I-LivingSituation',15:'B-Residence',16:'I-Residence'}\n","id_label_event={0:'Not Present',1:'Present'}\n","label_id_status = {v: k for k, v in id_label_status.items()}\n","label_id_method = {v: k for k, v in id_label_method.items()}\n","label_id_role = {v: k for k, v in id_label_role.items()}\n","label_id_ent = {v: k for k, v in id_label_ent.items()}\n","label_id_event = {v: k for k, v in id_label_event.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4O8hKL_E0Kf"},"outputs":[],"source":["special_tokens_dict = {'additional_special_tokens': ['[SPAN1_START]', '[SPAN1_END]', '[SPAN2_START]', '[SPAN2_END]']}\n","num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1707359216926,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"},"user_tz":300},"id":"AcEwvYtFxaWi","outputId":"3f02d151-f7d8-484c-c03b-7668138baf63"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[SPAN1_END]', '[SPAN2_END]', '[SPAN2_START]', '[SPAN1_START]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28996: AddedToken(\"[SPAN1_END]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28997: AddedToken(\"[SPAN2_END]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28998: AddedToken(\"[SPAN2_START]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t28999: AddedToken(\"[SPAN1_START]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":38}],"source":["tokenizer"]},{"cell_type":"markdown","source":["### Generate Relation labels"],"metadata":{"id":"Djb8CgGeaWCW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljz070mSDaGb"},"outputs":[],"source":["def insert_span_markers(text, spans):\n","    # Create a list to store the markers that need to be inserted at each index\n","    insertions = {i: [] for i in range(len(text) + 1)}\n","\n","    # Populate the insertions dictionary with the correct markers for each span\n","    sp_cnt=0\n","    for span in spans:\n","        sp_cnt=sp_cnt+1\n","        start, end, category = span\n","        start=int(start)\n","        end=int(end)\n","        insertions[start].append('[SPAN'+str(sp_cnt)+'_START]')\n","        insertions[end].append('[SPAN'+str(sp_cnt)+'_END]')\n","        '''\n","        if category == 'entity':\n","            insertions[start].append('[SPAN1_START]')\n","            insertions[end].append('[SPAN1_END]')\n","        elif category == 'role':\n","            insertions[start].append('[SPAN2_START]')\n","            insertions[end].append('[SPAN2_END]')\n","        '''\n","    # Construct the new text with markers\n","    new_text_pieces = []\n","    for i, char in enumerate(text):\n","        # Add markers before the current character\n","        if insertions[i]:\n","            new_text_pieces.append(' '+' '.join(insertions[i]) + ' ')\n","        new_text_pieces.append(char)\n","    # Add any markers that should be inserted after the last character\n","    if insertions[len(text)]:\n","        new_text_pieces.append(' ' + ' '.join(insertions[len(text)]))\n","\n","    # Join all pieces of the new text\n","    return ''.join(new_text_pieces)\n","# generate pairs of entity positions and role positions with label relation present and not present\n","def generate_position_pairs(data):\n","    '''\n","    Reads raw processed data and creates relations pairs, formulates input text and labels.\n","    Adds special tokens SPAN1_START,SPAN1_END (before and after the entity span) SPAN2_START,SPAN2_END (before and after the role span)\n","    '''\n","    entity_dict = {e['entity_id']: (e['entity_strt_pos'], e['entity_end_pos']) for e in data['entity_list']}\n","    role_dict = {r['role_id']: (r['entity_strt_pos'], r['entity_end_pos']) for r in data['role_list']}\n","    sentence=data['text']\n","    text_special_list = []\n","    pairs = []\n","    for event in data['events_list']:\n","        entity_pos = list(entity_dict.get(event['entity_id'], None))\n","        entity_pos.append('entity')\n","        related_role_positions = [role_dict[rid] for rid in event['Related_roles'] if rid in role_dict]\n","\n","        for role_pos in related_role_positions:\n","            role_pos=list(role_pos)\n","            role_pos.append('role')\n","            marked_sentence = insert_span_markers(sentence,[entity_pos,role_pos])\n","            #marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\n","            text_special_list.append({'Text': marked_sentence, 'Relation': label_id_event['Present']})\n","\n","            pairs.append({'Entity_Position': entity_pos, 'Role_Position': role_pos, 'Relation': 'Present'})\n","\n","        not_related_roles = set(role_dict.keys()) - set(event['Related_roles'])\n","        for role_id in not_related_roles:\n","            role_pos = list(role_dict[role_id])\n","            role_pos.append('role')\n","            marked_sentence = insert_span_markers(sentence,[entity_pos,role_pos])\n","            #marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\n","            text_special_list.append({'Text': marked_sentence, 'Relation': label_id_event['Not Present'] })\n","            pairs.append({'Entity_Position': entity_pos, 'Role_Position': role_pos, 'Relation': 'Not Present'})\n","\n","    return text_special_list\n"]},{"cell_type":"code","source":["'''\n","old method\n","# generate pairs of entity positions and role positions with label relation present and not present\n","def generate_position_pairs(data):\n","\n","    #Reads raw processed data and creates relations pairs, formulates input text and labels.\n","    #Adds special tokens SPAN1_START,SPAN1_END (before and after the entity span) SPAN2_START,SPAN2_END (before and after the role span)\n","\n","    entity_dict = {e['entity_id']: (e['entity_strt_pos'], e['entity_end_pos']) for e in data['entity_list']}\n","    role_dict = {r['role_id']: (r['entity_strt_pos'], r['entity_end_pos']) for r in data['role_list']}\n","    sentence=data['text']\n","    text_special_list = []\n","    pairs = []\n","    for event in data['events_list']:\n","        entity_pos = entity_dict.get(event['entity_id'], None)\n","\n","        related_role_positions = [role_dict[rid] for rid in event['Related_roles'] if rid in role_dict]\n","\n","        for role_pos in related_role_positions:\n","            marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\n","            text_special_list.append({'Text': marked_sentence, 'Relation': label_id_event['Present']})\n","\n","            pairs.append({'Entity_Position': entity_pos, 'Role_Position': role_pos, 'Relation': 'Present'})\n","\n","        not_related_roles = set(role_dict.keys()) - set(event['Related_roles'])\n","        for role_id in not_related_roles:\n","            role_pos = role_dict[role_id]\n","            marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\n","            text_special_list.append({'Text': marked_sentence, 'Relation': label_id_event['Not Present'] })\n","            pairs.append({'Entity_Position': entity_pos, 'Role_Position': role_pos, 'Relation': 'Not Present'})\n","\n","    return text_special_list\n","'''\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"VZK0AhPoWoW5","executionInfo":{"status":"ok","timestamp":1707359216927,"user_tz":300,"elapsed":15,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"002d3a84-69ce-4848-b928-f80e934e9ad5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nold method\\n# generate pairs of entity positions and role positions with label relation present and not present\\ndef generate_position_pairs(data):\\n    \\n    #Reads raw processed data and creates relations pairs, formulates input text and labels.\\n    #Adds special tokens SPAN1_START,SPAN1_END (before and after the entity span) SPAN2_START,SPAN2_END (before and after the role span)\\n    \\n    entity_dict = {e[\\'entity_id\\']: (e[\\'entity_strt_pos\\'], e[\\'entity_end_pos\\']) for e in data[\\'entity_list\\']}\\n    role_dict = {r[\\'role_id\\']: (r[\\'entity_strt_pos\\'], r[\\'entity_end_pos\\']) for r in data[\\'role_list\\']}\\n    sentence=data[\\'text\\']\\n    text_special_list = []\\n    pairs = []\\n    for event in data[\\'events_list\\']:\\n        entity_pos = entity_dict.get(event[\\'entity_id\\'], None)\\n        \\n        related_role_positions = [role_dict[rid] for rid in event[\\'Related_roles\\'] if rid in role_dict]\\n\\n        for role_pos in related_role_positions:\\n            marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\\n            text_special_list.append({\\'Text\\': marked_sentence, \\'Relation\\': label_id_event[\\'Present\\']})\\n\\n            pairs.append({\\'Entity_Position\\': entity_pos, \\'Role_Position\\': role_pos, \\'Relation\\': \\'Present\\'})\\n\\n        not_related_roles = set(role_dict.keys()) - set(event[\\'Related_roles\\'])\\n        for role_id in not_related_roles:\\n            role_pos = role_dict[role_id]\\n            marked_sentence = f\"{sentence[:int(entity_pos[0])]} [SPAN1_START] {sentence[int(entity_pos[0]):int(entity_pos[1])]} [SPAN1_END] {sentence[int(entity_pos[1]):int(role_pos[0])]} [SPAN2_START] {sentence[int(role_pos[0]):int(role_pos[1])]} [SPAN2_END] {sentence[int(role_pos[1]):]}\"\\n            text_special_list.append({\\'Text\\': marked_sentence, \\'Relation\\': label_id_event[\\'Not Present\\'] })\\n            pairs.append({\\'Entity_Position\\': entity_pos, \\'Role_Position\\': role_pos, \\'Relation\\': \\'Not Present\\'})\\n\\n    return text_special_list\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdPrt9pjoWCG"},"outputs":[],"source":["trainset=[]\n","testset=[]\n","with open(train_dataset_path,'r') as f:\n","  train_data=json.load(f)\n","  for data in train_data:\n","    trainset.extend(generate_position_pairs(data))\n","with open(test_dataset_path,'r') as f:\n","  test_data=json.load(f)\n","  for data in test_data:\n","    testset.extend(generate_position_pairs(data))"]},{"cell_type":"markdown","source":["### Dataset and data loader"],"metadata":{"id":"2l_yDFSxbaZY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXsDyqB9rPWZ"},"outputs":[],"source":["class RelationDataset(Dataset):\n","    def __init__(self, data,tokenizer,max_len):\n","      self.sentences = [item['Text'] for item in data]\n","      self.labels = [item['Relation'] for item in data]\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        sentence = self.sentences[idx]\n","        inputs=tokenizer(sentence, add_special_tokens=True, padding='max_length',max_length=max_len , truncation=True)\n","        inputs['labels']=self.labels[idx]\n","        return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GDNiBb-FHT6k"},"outputs":[],"source":["train_dataset = RelationDataset(trainset,tokenizer,max_len)\n","test_dataset =  RelationDataset(testset,tokenizer,max_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YmETO5O6pbs"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epe7cabrINEN"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=data_collator)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=data_collator)"]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"rwr3H0bZbDLr"}},{"cell_type":"markdown","source":["### Training args and Model Initialization"],"metadata":{"id":"cs6DZ_e1bTS7"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1524,"status":"ok","timestamp":1707359219565,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"},"user_tz":300},"id":"qwL62EVNesuA","outputId":"f9571773-4654-4a5e-d89c-074033db7cc5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":47}],"source":["model = BertForSequenceClassification.from_pretrained(bert_model_name,num_labels=2)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1707359220305,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"},"user_tz":300},"id":"4gZVUxP2gMz5","outputId":"92a26992-fba0-49d3-c529-7c3125b53225"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["optimizer=AdamW(model.parameters(),lr=learning_rate)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*num_train_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRKmwqGbg-u4"},"outputs":[],"source":["training_args={\n","    'output_dir':save_model_path,\n","    'num_train_epochs':num_train_epochs,\n","    'optimizer':optimizer,\n","    'scheduler':scheduler,\n","    'patience':patience,\n","    'run_name':'Relation_model_v1'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1707359220305,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"},"user_tz":300},"id":"mjHgO-MBqLm2","outputId":"7c32186a-9197-41c5-f9e9-f9a33397cc6f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(29000, 768)"]},"metadata":{},"execution_count":50}],"source":["model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1707359220305,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"},"user_tz":300},"id":"luaq5QXdv0_e","outputId":"e66adda5-aa5b-4641-8c7b-cf49f7b4fd4c"},"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([16, 512])\n","torch.Size([16])\n"]}],"source":["for batch in train_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    print(batch['input_ids'].shape)\n","    print(batch['labels'].shape)\n","    break"]},{"cell_type":"markdown","source":["### Training and evaluation"],"metadata":{"id":"bsf6UnQKbJcb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":916,"referenced_widgets":["e1eecdd415bc465e940f5e7c948d4e75","cbf7690d59094a99b06df4db2533d13e","2ce52d5e1ba343d09cbff70e5488460b","0abb172fca5f40609abd0e4b4fa09fbd","2dd0a26764624d0489ed966296953005","dd95b4b2f3c540f3ac5b98d703f69d53","6f5b3c7f99534d19bbaef2cb6045b2c6","af1ef4332b6942458a5a4422f9adc1cf","b6efd4239d8a4640859559d3378045af","457e3ddb7a30439dbf31da9415d6a1f4","36afcf006b8e4a6088b65c0020ca3eb2"]},"id":"CL1YiJQJ7O8z","executionInfo":{"status":"ok","timestamp":1707360812983,"user_tz":300,"elapsed":1592686,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"3aa8eae1-4309-480a-ce00-9febf169ccc2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/6470 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1eecdd415bc465e940f5e7c948d4e75"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: {} 0\n","training loss 173.04141235351562\n","Validation loss 17.390546798706055\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.98      1938\n","           1       0.88      0.96      0.92       539\n","\n","    accuracy                           0.96      2477\n","   macro avg       0.94      0.96      0.95      2477\n","weighted avg       0.97      0.96      0.96      2477\n","\n","Epoch: {} 1\n","training loss 47.502140045166016\n","Validation loss 9.817044258117676\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99      1938\n","           1       0.96      0.94      0.95       539\n","\n","    accuracy                           0.98      2477\n","   macro avg       0.97      0.97      0.97      2477\n","weighted avg       0.98      0.98      0.98      2477\n","\n","Epoch: {} 2\n","training loss 25.79714584350586\n","Validation loss 15.262432098388672\n","              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.98      1938\n","           1       0.98      0.90      0.94       539\n","\n","    accuracy                           0.97      2477\n","   macro avg       0.98      0.95      0.96      2477\n","weighted avg       0.98      0.97      0.97      2477\n","\n","Epoch: {} 3\n","training loss 19.97077178955078\n","Validation loss 10.707073211669922\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.99      0.99      1938\n","           1       0.96      0.94      0.95       539\n","\n","    accuracy                           0.98      2477\n","   macro avg       0.97      0.97      0.97      2477\n","weighted avg       0.98      0.98      0.98      2477\n","\n","Epoch: {} 4\n","training loss 13.356378555297852\n","Validation loss 10.614435195922852\n"]}],"source":["\n","from tqdm.auto import tqdm\n","from sklearn.metrics import classification_report\n","\n","progress_bar = tqdm(range(len(train_dataloader)*num_train_epochs))\n","early_stopping_cnt=0\n","val_loss_list=[]\n","for epoch in range(num_train_epochs):\n","  loss_epoch=0\n","  model.train()\n","  for batch in train_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    outputs = model(**batch)\n","    loss = outputs.loss\n","    loss_epoch+=loss\n","    loss.backward()\n","\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","    progress_bar.update(1)\n","  print(\"Epoch: {}\",epoch)\n","  print(\"training loss {}\".format(loss_epoch.item()))\n","\n","\n","  model.eval()\n","  loss_epoch_val=0\n","  val_predictions=[]\n","  true_labels=[]\n","  for batch in test_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","        val_loss=outputs.loss\n","        loss_epoch_val+=val_loss\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    val_predictions.extend(predictions.tolist())\n","    true_labels.extend(batch['labels'].tolist())\n","  print(\"Validation loss {}\".format(loss_epoch_val.item()))\n","  val_loss_list.append(loss_epoch_val.item())\n","  if epoch == 0:\n","    best_val_loss=loss_epoch_val.item()\n","    early_stopping_cnt=0\n","  else:\n","    if loss_epoch_val.item()<best_val_loss:\n","      best_val_loss=loss_epoch_val.item()\n","      early_stopping_cnt=0\n","    else:\n","      early_stopping_cnt+=1\n","  if early_stopping_cnt>=patience:\n","    break\n","  print(classification_report(true_labels, val_predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwK5Av-mNWbS"},"outputs":[],"source":["sv_pth=project_directory+'models/final_models/Indepent_relation_classifier_v'+str(ver)+'/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TOEEvlKPnpN"},"outputs":[],"source":["import os\n","if not os.path.exists(sv_pth):\n","    os.makedirs(sv_pth)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jj8WMzgb_SCr"},"outputs":[],"source":["model.save_pretrained(sv_pth)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uk4G6NeIKcjn","executionInfo":{"status":"ok","timestamp":1707360814134,"user_tz":300,"elapsed":197,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"3d75af1e-6358-4a6c-bfd3-4aed4204a953"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/PHD_assessment_gmu/models/final_models/Indepent_relation_classifier_v6/tokenizer_config.json',\n"," '/content/drive/MyDrive/PHD_assessment_gmu/models/final_models/Indepent_relation_classifier_v6/special_tokens_map.json',\n"," '/content/drive/MyDrive/PHD_assessment_gmu/models/final_models/Indepent_relation_classifier_v6/vocab.txt',\n"," '/content/drive/MyDrive/PHD_assessment_gmu/models/final_models/Indepent_relation_classifier_v6/added_tokens.json',\n"," '/content/drive/MyDrive/PHD_assessment_gmu/models/final_models/Indepent_relation_classifier_v6/tokenizer.json')"]},"metadata":{},"execution_count":56}],"source":["tokenizer.save_pretrained(sv_pth)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"_6Km5l-XkYrG","outputId":"2d1aec24-9068-41c6-ebbd-a76cf4c8e59e","executionInfo":{"status":"ok","timestamp":1707360814135,"user_tz":300,"elapsed":190,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom tqdm.auto import tqdm\\nfrom sklearn.metrics import classification_report\\n\\nprogress_bar = tqdm(range(len(train_dataloader)*num_train_epochs))\\nearly_stopping_cnt=0\\nbest_val_loss=0\\nval_loss_list=[]\\nfor epoch in range(num_train_epochs):\\n  loss_epoch=0\\n  model.train()\\n  for batch in train_dataloader:\\n    batch = {k: v.to(device) for k, v in batch.items()}\\n    outputs = model(**batch)\\n    loss = outputs.loss\\n    loss_epoch+=loss\\n    loss.backward()\\n\\n    optimizer.step()\\n    scheduler.step()\\n    optimizer.zero_grad()\\n    progress_bar.update(1)\\n  print(\"training loss {}\".format(loss_epoch.item()))\\n\\n\\n  model.eval()\\n  loss_epoch_val=0\\n  val_predictions=[]\\n  true_labels=[]\\n  for batch in test_dataloader:\\n    batch = {k: v.to(device) for k, v in batch.items()}\\n    with torch.no_grad():\\n        outputs = model(**batch)\\n        val_loss=outputs.loss\\n        loss_epoch_val+=val_loss\\n    logits = outputs.logits\\n    predictions = torch.argmax(logits, dim=-1)\\n    val_predictions.extend(predictions.tolist())\\n    true_labels.extend(batch[\\'labels\\'].tolist())\\n  print(\"Validation loss {}\".format(loss_epoch_val.item()))\\n  val_loss_list.append(loss_epoch_val.item())\\n  if epoch == 0:\\n    best_val_loss=loss_epoch_val.item()\\n    early_stopping_cnt=0\\n  else:\\n    if loss_epoch_val.item()<best_val_loss:\\n      best_val_loss=loss_epoch_val.item()\\n      early_stopping_cnt=0\\n    else:\\n      early_stopping_cnt+=1\\n  if early_stopping_cnt>=patience:\\n    break\\n\\n  print(classification_report(true_labels, val_predictions))\\n  '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["'''\n","from tqdm.auto import tqdm\n","from sklearn.metrics import classification_report\n","\n","progress_bar = tqdm(range(len(train_dataloader)*num_train_epochs))\n","early_stopping_cnt=0\n","best_val_loss=0\n","val_loss_list=[]\n","for epoch in range(num_train_epochs):\n","  loss_epoch=0\n","  model.train()\n","  for batch in train_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    outputs = model(**batch)\n","    loss = outputs.loss\n","    loss_epoch+=loss\n","    loss.backward()\n","\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","    progress_bar.update(1)\n","  print(\"training loss {}\".format(loss_epoch.item()))\n","\n","\n","  model.eval()\n","  loss_epoch_val=0\n","  val_predictions=[]\n","  true_labels=[]\n","  for batch in test_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","        val_loss=outputs.loss\n","        loss_epoch_val+=val_loss\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    val_predictions.extend(predictions.tolist())\n","    true_labels.extend(batch['labels'].tolist())\n","  print(\"Validation loss {}\".format(loss_epoch_val.item()))\n","  val_loss_list.append(loss_epoch_val.item())\n","  if epoch == 0:\n","    best_val_loss=loss_epoch_val.item()\n","    early_stopping_cnt=0\n","  else:\n","    if loss_epoch_val.item()<best_val_loss:\n","      best_val_loss=loss_epoch_val.item()\n","      early_stopping_cnt=0\n","    else:\n","      early_stopping_cnt+=1\n","  if early_stopping_cnt>=patience:\n","    break\n","\n","  print(classification_report(true_labels, val_predictions))\n","  '''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9LmFaZMnlZ9","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1707360814135,"user_tz":300,"elapsed":147,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"519f1626-0258-4d3e-9a28-3ea3d36d3845"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass RelationClassifier(nn.Module):\\n    def __init__(self, model_name, tokenizer,num_labels):\\n        super(RelationClassifier, self).__init__()\\n        self.tokenizer = tokenizer\\n\\n        self.bert = BertModel.from_pretrained(model_name)\\n        self.bert.resize_token_embeddings(len(self.tokenizer))\\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\\n\\n    def forward(self, input_ids, attention_mask=None):\\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\\n        pooled_output = outputs[1]\\n        logits = self.classifier(pooled_output)\\n        return logits\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}],"source":["'''\n","class RelationClassifier(nn.Module):\n","    def __init__(self, model_name, tokenizer,num_labels):\n","        super(RelationClassifier, self).__init__()\n","        self.tokenizer = tokenizer\n","\n","        self.bert = BertModel.from_pretrained(model_name)\n","        self.bert.resize_token_embeddings(len(self.tokenizer))\n","        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        logits = self.classifier(pooled_output)\n","        return logits\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNw_e119H0DP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"V100","mount_file_id":"1dx0wABkMrf31hzQsxlVJV_dfZeIXeBbn","authorship_tag":"ABX9TyP2tZ4kwh7VSlgwRbYUps1g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e1eecdd415bc465e940f5e7c948d4e75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbf7690d59094a99b06df4db2533d13e","IPY_MODEL_2ce52d5e1ba343d09cbff70e5488460b","IPY_MODEL_0abb172fca5f40609abd0e4b4fa09fbd"],"layout":"IPY_MODEL_2dd0a26764624d0489ed966296953005"}},"cbf7690d59094a99b06df4db2533d13e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd95b4b2f3c540f3ac5b98d703f69d53","placeholder":"​","style":"IPY_MODEL_6f5b3c7f99534d19bbaef2cb6045b2c6","value":" 50%"}},"2ce52d5e1ba343d09cbff70e5488460b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_af1ef4332b6942458a5a4422f9adc1cf","max":6470,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6efd4239d8a4640859559d3378045af","value":3235}},"0abb172fca5f40609abd0e4b4fa09fbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_457e3ddb7a30439dbf31da9415d6a1f4","placeholder":"​","style":"IPY_MODEL_36afcf006b8e4a6088b65c0020ca3eb2","value":" 3235/6470 [26:08&lt;19:06,  2.82it/s]"}},"2dd0a26764624d0489ed966296953005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd95b4b2f3c540f3ac5b98d703f69d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f5b3c7f99534d19bbaef2cb6045b2c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af1ef4332b6942458a5a4422f9adc1cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6efd4239d8a4640859559d3378045af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"457e3ddb7a30439dbf31da9415d6a1f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36afcf006b8e4a6088b65c0020ca3eb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}