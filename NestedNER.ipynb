{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1AFKb81GKTXPydliNu3RZNlxVWsOAhHZY","authorship_tag":"ABX9TyOinWYE0ZN3d3I5b+m8nrJG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Nested Ner Model\n","### **Description:**\n","\n","\n","*   Solving nested entity problems for type entities.\n","*   Family and Living situation are major entities which has nested structure.\n","\n","\n","*   Generated 2 levels of labels to handle nested entities. Identifies independent and nested token positions and labels them in following way:\n","    1.  Level1: Labels comprise of independent and outer most entities of nested structure.\n","    2.  Level2: Labels comprise of inner layer of nested structure and if a nested depth is greater than 2 then the priority order will decide.\n","*   Created BERT-CRF architecture with 2 classification heads (linear+CRF)\n","### **Experimentation:**\n","*  **Approach 1**: Two model architectures are developed, with Model 1 designed to fine-tune both Level 1 and Level 2 using sentence-wide labels as described previously. A potential concern arises regarding Level 2's ability to accurately learn nested entities, given that the input information encompasses not just nested entities but also independent and outermost entities, possibly introducing complexity or ambiguity in distinguishing nested relationships.\n","\n","*   **Approach 2 (Nested NER Span model):** operates by taking the spans of the outermost entities as inputs for Level 2. The underlying ideology posits that by focusing on the spans of the outermost entities at level 1, the model can more effectively discern and learn the innermost entities at level 2, thereby reducing potential confusion. Furthermore, this model is designed to aggregate spans pertaining to nested entities and accurately map them back to their respective positions.\n","\n","### **Model**\n","\n","1.  Tokenizer: BertTokenizerFast\n","2.  pre-trained Bert model: 'emilyalsentzer/Bio_ClinicalBERT'\n","3.  Hyperparameters:\n","    *   eps=1e-8\n","    * learning_rate=7e-5\n","    * weight_decay=0\n","    * num_train_epochs=15\n","    * patience=3\n","    * batch_size=8\n","    * max_len_token=512\n","\n","\n","4.  Fine-tuned a pre-trained Bert model with CRF and multiple classification heads where each (Independent, outermost) and nested entities.\n","5.  Since we are training multiple tasks simultaneously, we sum the losses and send them for backpropagation.\n","6.  Sequence evaluation is used as NER metrics.\n","7.  Model stored at project_directory+'/models/final_models/Nested_Entity_CRFModel_v1.pth'.\n","8.\n","\n","### **Inputs:**\n","train_data_set path: 'PHD_assessment_gmu/data/train_dataset.pth'\n","test_data_set path: 'PHD_assessment_gmu/data/test_dataset.pth'\n","\n","**Run:**\n","change the project directory and press run all.\n","###**Metrics:**\n","\n","{'Alcohol': {'precision': 0.7368421052631579, 'recall': 0.7636363636363637, 'f1': 0.7499999999999999, 'number': 55}, 'Drug': {'precision': 0.8, 'recall': 0.9333333333333333, 'f1': 0.8615384615384616, 'number': 30}, 'Family': {'precision': 0.4827586206896552, 'recall': 0.7, 'f1': 0.5714285714285714, 'number': 20}, 'LivingSituation': {'precision': 0.48, 'recall': 0.631578947368421, 'f1': 0.5454545454545454, 'number': 19}, 'MaritalStatus': {'precision': 0.4722222222222222, 'recall': 0.5666666666666667, 'f1': 0.5151515151515152, 'number': 30}, 'Occupation': {'precision': 0.29411764705882354, 'recall': 0.3448275862068966, 'f1': 0.31746031746031744, 'number': 29}, 'Tobacco': {'precision': 0.8070175438596491, 'recall': 0.8679245283018868, 'f1': 0.8363636363636363, 'number': 53}, 'overall_precision': 0.6190476190476191, 'overall_recall': 0.7161016949152542, 'overall_f1': 0.6640471512770137, 'overall_accuracy': 0.9268861054471308}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6818181818181818, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7692307692307693, 'overall_accuracy': 0.9976696766676376}\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"fZ640jqxScgQ"}},{"cell_type":"markdown","source":["### Library installation"],"metadata":{"id":"c6cfODhfEGmp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4XdOjKaaJh5"},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","source":["!pip install seqeval"],"metadata":{"id":"wSh-VG8RarTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pytorch-crf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6q8Ff4ONGClI","executionInfo":{"status":"ok","timestamp":1706923849322,"user_tz":300,"elapsed":5223,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"e586819f-3734-4cae-bf0b-903455c5871b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.10/dist-packages (0.7.2)\n"]}]},{"cell_type":"markdown","source":["### Importing"],"metadata":{"id":"dKmR5cwnSVAa"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import evaluate\n","from transformers import BertTokenizerFast, BertModel, AdamW, get_linear_schedule_with_warmup, DataCollatorForTokenClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm, trange\n","from torchcrf import CRF"],"metadata":{"id":"N9xWjksNawgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric=evaluate.load(\"seqeval\")\n","device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_freeze_layers=6"],"metadata":{"id":"kMCs2RLja1-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["project_directory='/content/drive/MyDrive/PHD_assessment_gmu/'"],"metadata":{"id":"l0QG0CBqiwfO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Hyperparameters and data paths"],"metadata":{"id":"sfRmTX1wcqy4"}},{"cell_type":"code","source":["eps=1e-8\n","learning_rate=7e-5\n","weight_decay=0.01\n","num_train_epochs=8\n","patience=5\n","batch_size=8\n","max_len=512\n","min_label_size=10\n","\n","discarded_enities=['EnvironmentalExposure','SexualHistory','InfectiousDiseases','PhysicalActivity','Residence']\n","discarded_roles=['LivingStatus','Other','MedicalCondition','Extent','History']\n","\n","\n","raw_dataset_path=project_directory+'data/'+'SocialHistoryMTSamples.json'\n","train_dataset_path=project_directory+'data/'+'train_dataset.pth'\n","test_dataset_path=project_directory+'data/'+'test_dataset.pth'\n","bert_model_name='emilyalsentzer/Bio_ClinicalBERT'\n","priority_order = ['LivingSituation', 'Family', 'MaritalStatus']\n","tokenizer = BertTokenizerFast.from_pretrained(bert_model_name)\n","save_model_path=project_directory+'/models/'"],"metadata":{"id":"t-j924pUa6CS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Data Loading"],"metadata":{"id":"Sc8RnsL0c3iU"}},{"cell_type":"code","source":["label_id_l1={'B-Alcohol':1,\n"," 'B-Drug':3,\n"," 'B-Family':5,\n"," 'B-LivingSituation':7,\n"," 'B-MaritalStatus':9,\n"," 'B-Occupation':11,\n","\n"," 'B-Tobacco':13,\n"," 'I-Alcohol':2,\n"," 'I-Drug':4,\n"," 'I-Family':6,\n"," 'I-LivingSituation':8,\n"," 'I-MaritalStatus':10,\n"," 'I-Occupation':12,\n","\n"," 'I-Tobacco':14,\n"," 'O':0}\n","id_label_l1 = {v: k for k, v in label_id_l1.items()}\n","label_id_l2={'B-Family':1,\n"," 'B-LivingSituation':3,\n","\n"," 'I-Family':2,\n"," 'I-LivingSituation':4,\n","\n"," 'O':0}\n","id_label_l2 = {v: k for k, v in label_id_l2.items()}"],"metadata":{"id":"Ue3uP20Ia-YM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l2_entities=['Family','LivingSituation']"],"metadata":{"id":"5aEbP0OvkjWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(project_directory+'data/'+'trainset.json', 'r', encoding='utf-8') as file:\n","  train_data=json.load(file)\n","with open(project_directory+'data/'+'testset.json', 'r', encoding='utf-8') as file:\n","  test_data=json.load(file)"],"metadata":{"id":"tjKQQA-_bVxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","with open('/content/drive/MyDrive/PHD_assessment_gmu/data/nested_ent_filenames.json', 'r', encoding='utf-8') as file:\n","  nested_ent_filenames=json.load(file)\n","nested_lst_tr=[]\n","nested_lst_tst=[]\n","for ele in train_data:\n","  if ele['file_name'] in nested_ent_filenames['train_files']:\n","    nested_lst_tr.append(ele)\n","for ele in test_data:\n","  if ele['file_name'] in nested_ent_filenames['test_files']:\n","    nested_lst_tst.append(ele)"],"metadata":{"id":"m6UMHp9acYXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate nested entity Labels"],"metadata":{"id":"ON2F7OGhc8Fp"}},{"cell_type":"code","source":["def generate_nested_bio_labels_with_prioritization_and_positions(text, entity_list, priority_order,tokenizer):\n","    '''\n","   The function organizes entities into a hierarchical two-layer structure, discerning nested entities where the outer entities are\n","   labeled in the first layer and the inner entities in the second.\n","   Single-level entities are exclusively labeled in the first layer.\n","   The function is designed to handle up to two layers of nesting; for more complex nesting beyond two layers, a priority order is employed\n","   to determine the labeling.\n","    '''\n","    # Tokenize the text\n","    tokenized_output = tokenizer(text,max_length=max_len,truncation=True,return_offsets_mapping=True)\n","    tokens = tokenized_output.tokens()\n","    layer1_labels = ['O'] * len(tokens)  # Layer 1\n","    layer2_labels = ['O'] * len(tokens)  # Layer 2\n","    tokenized_output['tokens']=tokens\n","\n","    def update_labels(labels, token_idx, label, is_begin, priority):\n","        '''\n","        Update labels when the new label has a higher priority.\n","        '''\n","        current_label = labels[token_idx].split('-')[-1] if labels[token_idx] != 'O' else None\n","        current_priority = priority_order.index(current_label) if current_label in priority_order else len(priority_order)\n","        if current_label is None or priority < current_priority:\n","            prefix = 'B-' if is_begin else 'I-'  # 'B-' for Begin, 'I-' for Inside\n","            labels[token_idx] = prefix + label\n","\n","    def is_nested(entity, other_entities):\n","        '''\n","        Determines whether an entity is nested\n","        '''\n","        entity_start = int(entity['entity_strt_pos'])\n","        entity_end = int(entity['entity_end_pos'])\n","        for other in other_entities:\n","            if other == entity:\n","                continue\n","            other_start = int(other['entity_strt_pos'])\n","            other_end = int(other['entity_end_pos'])\n","            if other_start <= entity_start and other_end >= entity_end:\n","                return True\n","        return False\n","\n","    def has_nested_entities(entity, all_entities):\n","      '''\n","      Identifies whether an entity has nested entities or not.\n","      '''\n","      return any(is_nested(other, [entity]) for other in all_entities if other != entity)\n","    # To store start and end positions of encompassing entities\n","    encompassing_positions = []\n","    # Assign labels to the layers and track positions\n","    for entity in entity_list:\n","        entity_category = entity['entity_category']\n","        entity_start_char = int(entity['entity_strt_pos'])\n","        entity_end_char = int(entity['entity_end_pos'])\n","        start_token_idx = tokenized_output.char_to_token(entity_start_char)\n","        end_token_idx = tokenized_output.char_to_token(entity_end_char - 1)\n","        entity_priority = priority_order.index(entity_category) if entity_category in priority_order else len(priority_order)\n","\n","        if start_token_idx is not None and end_token_idx is not None:\n","            for token_idx in range(start_token_idx, end_token_idx + 1):\n","                is_begin = token_idx == start_token_idx\n","                if not is_nested(entity, entity_list):\n","                    # Non-nested or encompassing entities in Layer 1\n","                    update_labels(layer1_labels, token_idx, entity_category, is_begin, entity_priority)\n","                    if is_begin and has_nested_entities(entity, entity_list):\n","                        encompassing_positions.append((start_token_idx, end_token_idx+1))\n","                else:\n","                    # Nested entities in Layer 2, with priority\n","                    update_labels(layer2_labels, token_idx, entity_category, is_begin, entity_priority)\n","\n","    layer1_labels_updated=[label_id_l1['O'] if ele.split('-',1)[-1] in discarded_enities else label_id_l1[ele] for ele in layer1_labels]\n","    layer2_labels_updated=[label_id_l2[ele] if ele.split('-',1)[-1] in l2_entities else label_id_l2['O'] for ele in layer2_labels]\n","\n","    return layer1_labels_updated, layer2_labels_updated, encompassing_positions, tokenized_output\n","\n","\n"],"metadata":{"id":"Oav2D3Xu8wMt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset and Padding"],"metadata":{"id":"wnBDzJghiJ6D"}},{"cell_type":"code","source":["def pad_span_positions(span_positions):\n","  '''\n","  Padding the span positions with (-1,-1) for the batch\n","  '''\n","  max_length = max(len(spans) for spans in span_positions)\n","\n","# Pad the span positions\n","  padded_spans = []\n","  span_masks = []\n","  for spans in span_positions:\n","    # Pad with (-1, -1)\n","    if spans==[]:\n","      padded = [(-1, -1)] * max_length\n","    else:\n","      padded = spans + [(-1, -1)] * (max_length - len(spans))\n","    padded_spans.append(padded)\n","  return torch.tensor(padded_spans)\n","def collate_fn_entity_role(batch):\n","  '''\n","  Collate function for the entity role dataset\n","  '''\n","  input_ids = [torch.tensor(x['input_ids']) for x in batch]\n","  attention_mask = [torch.tensor(x['attention_mask']) for x in batch]\n","  l_1_labels = [torch.tensor(x['l_1_labels']) for x in batch]\n","  l_2_labels = [torch.tensor(x['l_2_labels']) for x in batch]\n","  tokens=[x['tokens'] for x in batch]\n","  outermost_entities_pos=[x['outermost_entities_pos'] for x in batch]\n","\n","  input_ids = pad_sequence(input_ids, batch_first=True,padding_value=tokenizer.pad_token_id)\n","  attention_mask = pad_sequence(attention_mask, batch_first=True,padding_value=0)\n","  l_1_labels = pad_sequence(l_1_labels, batch_first=True,padding_value=-100)\n","  l_2_labels = pad_sequence(l_2_labels, batch_first=True,padding_value=-100)\n","  outermost_entities_pos=pad_span_positions(outermost_entities_pos)\n","  return {\n","    'input_ids':input_ids,\n","    'attention_mask':attention_mask,\n","    'l_1_labels':l_1_labels,\n","    'l_2_labels':l_2_labels,\n","    'tokens':tokens,\n","    'outermost_entities_pos':outermost_entities_pos\n","  }\n"],"metadata":{"id":"nvpvbjoBRfWA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NestedEntityDataset(Dataset):\n","  def __init__(self, data, tokenizer, max_len):\n","    self.data = data\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  def __len__(self):\n","    return len(self.data)\n","  def __getitem__(self, idx):\n","    item=self.data[idx]\n","    text = item['text']\n","    level_1_lab,level_2_lab,outermost_entities_pos,inputs=generate_nested_bio_labels_with_prioritization_and_positions(text, item['entity_list'], priority_order,self.tokenizer)\n","    #inputs = self.tokenizer(text,max_length=max_len,truncation=True,return_offsets_mapping=True)\n","    tokens_len=len(inputs['input_ids'])\n","    offset_mapping_list=inputs['offset_mapping']\n","    #entity_labels=GenerateLabel.generate_enity_labels(item['entity_list'],tokens_len,offset_mapping_list)\n","    #role_labels, status_labels, method_labels=GenerateLabel.generate_role_labels(item['role_list'],tokens_len,offset_mapping_list)\n","    #relation_labels=GenerateLabel.generate_relation_labels(offset_mapping_list,item)\n","    return {\n","      'input_ids':inputs['input_ids'],\n","      'attention_mask':inputs['attention_mask'],\n","      'l_1_labels':level_1_lab,\n","      'l_2_labels':level_2_lab,\n","      'outermost_entities_pos':outermost_entities_pos,\n","      'text':text,\n","      'file_name':item['file_name'],\n","      'offset_mapping':offset_mapping_list,\n","      'tokens':inputs['tokens']\n","    }"],"metadata":{"id":"BfbPlWVkCE4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset=NestedEntityDataset(train_data,tokenizer,max_len)\n","test_dataset=NestedEntityDataset(test_data,tokenizer,max_len)"],"metadata":{"id":"_QKT5VpHJYBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_fn_entity_role)\n","test_dataloader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_entity_role)"],"metadata":{"id":"ebsnhtdBRVB0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### NER model with 2 layers processing whole sentence"],"metadata":{"id":"c3qDjPoEluSS"}},{"cell_type":"code","source":["class NestedNERModel(nn.Module):\n","    '''\n","    Created a BERT-CRF architecture model.\n","    Have 2 classification heads each one processes the whole sentence and tuned for its layered labels.\n","    level1: classifies independent entities and nested entities with the outermost class\n","    level2: nested entities with 2nd priority inner level if it has more than 2 or the 2nd nested entity\n","    '''\n","    def __init__(self, bert_model_name, num_labels_level1, num_labels_level2):\n","        super(NestedNERModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","\n","        self.level1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_level1)\n","        self.level2_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_level2)\n","        self.dropout = nn.Dropout(0.1)\n","        self.crf_level1 = CRF(num_labels_level1, batch_first=True)\n","        self.crf_level2 = CRF(num_labels_level2, batch_first=True)\n","\n","    def forward(self, input_ids, attention_mask, labels_level1=None, labels_level2=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        level1_feats = self.level1_classifier(sequence_output)\n","        level2_feats = self.level2_classifier(sequence_output)\n","        mask = attention_mask.type(torch.uint8)\n","        if labels_level1 is not None and labels_level2 is not None:\n","          labels_level1 = torch.where(mask == 1, labels_level1, torch.tensor(-1).to(labels_level1.device))\n","          labels_level2 = torch.where(mask == 1, labels_level2, torch.tensor(-1).to(labels_level2.device))\n","\n","          loss_level1 = -self.crf_level1(level1_feats, labels_level1, mask=mask,reduction='mean')\n","          loss_level2 = -self.crf_level2(level2_feats, labels_level2, mask=mask,reduction='mean')\n","          return loss_level1 + loss_level2\n","        else:\n","          prediction_level1 = self.crf_level1.decode(level1_feats, mask=attention_mask.byte())\n","          prediction_level2 = self.crf_level2.decode(level2_feats, mask=attention_mask.byte())\n","          return prediction_level1, prediction_level2\n"],"metadata":{"id":"EO5Zm7UvIYgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Nested Ner Span model."],"metadata":{"id":"Hk1vU_sHl8S-"}},{"cell_type":"code","source":["\n","def update_prediction_layer2(span_type_id,batch_size,predictions,labels):\n","  '''\n","  This function processes a batch of span predictions, mapping them back to the document level in a batch.\n","  It generates single-layer prediction labels for each token, updated comprehensively by the predictions from all spans relevant to a document.\n","  '''\n","  updated_pred_batch={}\n","  for sp_id,pred in zip(span_type_id,predictions):\n","    if sp_id[0] not in updated_pred_batch.keys():\n","      updated_pred_batch[sp_id[0]]=[0]*len(labels[sp_id[0]])\n","    updated_pred_batch[sp_id[0]][sp_id[1][0]:sp_id[1][1]]=pred\n","  batch_indexes=set(range(0,batch_size))\n","  missing_indexes=batch_indexes-set(list(updated_pred_batch.keys()))\n","  for ind in missing_indexes:\n","    updated_pred_batch[ind]=[0]*len(labels[ind])\n","  values_list = [updated_pred_batch[i] for i in range(0, len(updated_pred_batch))]\n","  return values_list"],"metadata":{"id":"_s2RZMPESYKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class NestedNERSpanModel(nn.Module):\n","    '''\n","    Model Architecture:\n","    Created a BERT-CRF architecture model.\n","    We have 2 classification heads with CRF layers where level1 classifies the whole sentence and level2 classifies the span of the outermost entity.\n","    level1: classifies independent entities and nested entities with the outermost class\n","    level2: nested entities with 2nd priority inner level if it has more than 2 or the 2nd nested entity\n","    '''\n","    def __init__(self, bert_model_name, num_labels_level1, num_labels_level2):\n","        super(NestedNERSpanModel, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        self.level1_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_level1)\n","        self.level2_classifier = nn.Linear(self.bert.config.hidden_size, num_labels_level2)\n","        self.dropout = nn.Dropout(0.1)\n","        self.crf_level1 = CRF(num_labels_level1, batch_first=True)\n","        self.crf_level2 = CRF(num_labels_level2, batch_first=True)\n","\n","    def forward(self, input_ids, attention_mask, span_positions, labels_level1=None, labels_level2=None):\n","        '''\n","        Perfomance sentence prediction for level1 and span prediction for level2\n","        Input for level2 is the bert embeddings calculated for the span of the outermost entity.\n","        Input for level1 is the bert embeddings calculated for the whole sentence.\n","        Span padding and masking for a batch is done here for level2 inputs.\n","        Also checks if a particular batch has a span or not. If not, it is skipped.\n","        '''\n","\n","\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs[0]\n","        drp_output = self.dropout(sequence_output)\n","\n","        #Predicting level1 labels for independent and outer-most entities.\n","        level1_feats = self.level1_classifier(drp_output)\n","\n","        batch_labels_2=[]\n","        span_emb_lst=[]\n","        span_mask_list=[]\n","        max_span_len=0\n","        span_type_id=[]\n","\n","        #Creation of level2 features\n","        for batch_ind in range(input_ids.size(0)):\n","          sen_span_pos=0\n","          for strt, end in span_positions[batch_ind]:\n","            #Ignores padded span positions.\n","            if strt == -1 and end == -1:\n","              continue\n","            span_len=end-strt\n","\n","            #creation of span embeddings\n","            max_span_len=max(max_span_len,span_len)\n","            span_embedding = sequence_output[batch_ind,strt:end,:]\n","            span_emb_lst.append(span_embedding)\n","\n","            #Extracting labels for level2 feats\n","            if labels_level2 is not None:\n","              batch_labels_2.append(labels_level2[batch_ind][strt:end])\n","            #creation of span masks for level2 features\n","            span_mask = torch.ones(span_len, dtype=torch.uint8, device=input_ids.device)\n","            span_mask_list.append(span_mask)\n","            #Collects spans offset position and maps the position of the sentence in a batch\n","            span_type_id.append((batch_ind,(strt,end)))\n","            sen_span_pos+=1\n","\n","        #Span padding and masking for a batch is done here for level2 inputs.\n","        if span_emb_lst != []:\n","          padded_span_embs = pad_sequence(span_emb_lst, batch_first=True, padding_value=0.0)\n","          padded_span_masks = pad_sequence(span_mask_list, batch_first=True, padding_value=0)\n","          padded_span_msk=padded_span_masks.type(torch.uint8)\n","          if labels_level2 is not None:\n","              padded_labels_level2 = pad_sequence(batch_labels_2, batch_first=True, padding_value=-100)\n","          level2_feats = self.level2_classifier(padded_span_embs)\n","        mask = attention_mask.type(torch.uint8)\n","\n","        #Prediction of level1 and level2\n","        if labels_level1 is not None and labels_level2 is not None:\n","          labels_level1 = torch.where(mask == 1, labels_level1, torch.tensor(-1).to(labels_level1.device))\n","          loss_level1 = -self.crf_level1(level1_feats, labels_level1, mask=mask,reduction='mean')\n","          if span_emb_lst != []:\n","            padded_labels_level2 = torch.where(padded_span_masks == 1, padded_labels_level2, torch.tensor(-1).to(padded_labels_level2.device))\n","            loss_level2 = -self.crf_level2(level2_feats, padded_labels_level2, mask=padded_span_msk,reduction='mean')\n","          else:\n","            #If a batch encounters no nested spans then loss is 0 for level2.\n","            loss_level2=0\n","          return loss_level1 + loss_level2\n","        else:\n","          prediction_level1 = self.crf_level1.decode(level1_feats, mask=mask)\n","          if span_emb_lst != []:\n","            prediction_level2 = self.crf_level2.decode(level2_feats, mask=padded_span_msk)\n","\n","            #Updates span predictions to the token positions in a sentence\n","            prediction_level2=update_prediction_layer2(span_type_id,input_ids.size(0),prediction_level2,prediction_level1)\n","\n","          else:\n","            prediction_level2 = [[0 for _ in sublist] for sublist in prediction_level1]\n","\n","          return prediction_level1, prediction_level2\n"],"metadata":{"id":"GJdC9MaQq5gw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=NestedNERSpanModel(bert_model_name, len(id_label_l1), len(id_label_l2))"],"metadata":{"id":"2gfw5GeMIzTu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Training and evaluation"],"metadata":{"id":"-Yro7x13RGfo"}},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=learning_rate,eps=eps)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*num_train_epochs)"],"metadata":{"id":"L88erJw9SXvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706923852359,"user_tz":300,"elapsed":9,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"d660fafe-2744-4bbf-8074-711adf34a7a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["training_args={\n","    'output_dir':save_model_path,\n","    'num_train_epochs':num_train_epochs,\n","    'optimizer':optimizer,\n","    'scheduler':scheduler,\n","    'patience':patience,\n","    'run_name':'Nested_Entity_CRFModel_v1'\n","}"],"metadata":{"id":"9TRJg1i0SySq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_ner_metric(preds,labels,id_label):\n","    ground_truths=[[id_label[l] for l in lab if l!=-100] for lab in labels]\n","    prediction_labels=[[id_label[p] for p,l in zip(predict,lab) if l!=-100] for predict,lab in zip(preds,labels)]\n","    metric_res=metric.compute(predictions=prediction_labels,references=ground_truths)\n","    return metric_res\n"],"metadata":{"id":"5JA3-O9h4EBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_entity_model(model,val_loader,id_label_l1,id_label_l2,extract_predictions=False,dt_set=''):\n","  model.eval()\n","  with torch.no_grad():\n","    all_res_l1=[]\n","    all_res_l2=[]\n","    l1_predlst=[]\n","    l2_predlst=[]\n","    l2_lab=[]\n","    l1_lab=[]\n","    val_loss=0\n","\n","    for step,batch in enumerate(val_loader):\n","      l_1_labels=batch['l_1_labels'].to(device)\n","      l_2_labels=batch['l_2_labels'].to(device)\n","      span_pos=batch['outermost_entities_pos'].to(device)\n","\n","      inputs={'input_ids':batch['input_ids'].to(device),'attention_mask':batch['attention_mask'].to(device),'span_positions':span_pos}\n","      #Model Computation\n","      batch_loss=model(input_ids=batch['input_ids'].to(device),attention_mask=batch['attention_mask'].to(device),span_positions=span_pos,labels_level1=l_1_labels, labels_level2=l_2_labels)\n","      #Compute loss\n","      val_loss+=batch_loss\n","      pred_l1,pred_l2=model(**inputs)\n","      l1_predlst.extend(pred_l1)\n","      l2_predlst.extend(pred_l2)\n","      l1_lab.extend(l_1_labels.tolist())\n","      l2_lab.extend(l_2_labels.tolist())\n","      #Extract predictions and map to token and label\n","      for tok_lst,lab_lst,pred_lst in zip(batch['tokens'],l_1_labels.tolist(),pred_l1):\n","        all_res_l1.append(list(zip(tok_lst,lab_lst,pred_lst)))\n","      for tok_lst,lab_lst,pred_lst in zip(batch['tokens'],l_2_labels.tolist(),pred_l2):\n","        all_res_l2.append(list(zip(tok_lst,lab_lst,pred_lst)))\n","    #Dump results\n","    all_res={'level_1':all_res_l1,'level_2':all_res_l2}\n","    if extract_predictions:\n","      with open('/content/drive/MyDrive/PHD_assessment_gmu/data/'+'nested_ner_predictions_'+dt_set+'.json','w') as f:\n","        json.dump(all_res,f)\n","    #Compute metrics\n","    l1_metrics=compute_ner_metric(l1_predlst,l1_lab,id_label_l1)\n","    l2_metrics=compute_ner_metric(l2_predlst,l2_lab,id_label_l2)\n","\n","  return {'val_loss':val_loss,'l1_metrics':l1_metrics,'l2_metrics':l2_metrics}\n","\n","\n"],"metadata":{"id":"K-34JQ6zTIXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_entity_bert_model(model,tr_dataloader,tst_dataloader,id_label_l1,id_label_l2,bert_model_name,training_args):\n","  if torch.cuda.is_available():\n","    model.cuda()\n","  train_cycles=trange(training_args['num_train_epochs'],desc='Epoch',disable=0)\n","\n","  optimizer=training_args['optimizer']\n","  scheduler=training_args['scheduler']\n","  patience=training_args['patience']\n","\n","  tr_ent_loss_lst=[]\n","  val_ent_loss_list=[]\n","\n","  early_stopping_count=0\n","  min_ent_val_loss=0\n","\n","  for cycle in train_cycles:\n","    epoch_cycles=tqdm(tr_dataloader,desc='Iteration',disable=-1)\n","    model.train()\n","    tr_ent_loss=0\n","\n","    for step,batch in enumerate(epoch_cycles):\n","      optimizer.zero_grad()\n","      l_1_labels=batch['l_1_labels'].to(device)\n","      l_2_labels=batch['l_2_labels'].to(device)\n","      span_pos=batch['outermost_entities_pos'].to(device)\n","      #Model Computation\n","      batch_loss=model(input_ids=batch['input_ids'].to(device),attention_mask=batch['attention_mask'].to(device),span_positions=span_pos,labels_level1=l_1_labels, labels_level2=l_2_labels)\n","      #Loss\n","      batch_loss.backward()\n","      optimizer.step()\n","      scheduler.step()\n","      tr_ent_loss+=batch_loss\n","    #Evaluation\n","    tr_ent_loss_lst.append(tr_ent_loss/len(tr_dataloader))\n","    if early_stopping_count == patience or early_stopping_count == patience-1:\n","      tr_results=evaluate_entity_model(model,tr_dataloader,id_label_l1,id_label_l2,True,'tr')\n","      val_results=evaluate_entity_model(model,tst_dataloader,id_label_l1,id_label_l2,True,'tst')\n","    else:\n","      val_results=evaluate_entity_model(model,tst_dataloader,id_label_l1,id_label_l2)\n","      tr_results=evaluate_entity_model(model,tr_dataloader,id_label_l1,id_label_l2)\n","\n","    val_ent_loss_list.append(val_results['val_loss'])\n","    print('Epoch: {}  Train Method Loss: {}'.format(cycle,tr_ent_loss_lst[-1]))\n","\n","    print('Epoch: {}  Val Method Loss: {}'.format(cycle,val_ent_loss_list[-1]))\n","    print('Method_metrics: \\n')\n","    print(val_results['l1_metrics'])\n","    print('\\n')\n","    print(val_results['l2_metrics'])\n","    print('\\n')\n","    #Early stopping\n","    if cycle==0:\n","      min_ent_val_loss=val_ent_loss_list[-1]\n","      early_stopping_count=0\n","    else:\n","      if val_ent_loss_list[-1]<min_ent_val_loss:\n","        min_ent_val_loss=val_ent_loss_list[-1]\n","        early_stopping_count=0\n","      else:\n","        early_stopping_count+=1\n","      if early_stopping_count>=patience:\n","        print('Early stopping counter for method model : {}'.format(early_stopping_count))\n","        break\n","  #Save model\n","  torch.save(model.state_dict(),training_args['output_dir']+training_args['run_name']+'.pth')\n","  return model , {'train_loss':{'tr_method_loss_lst':tr_ent_loss_lst}\n","                  ,'val_loss':{'val_method_loss_list':val_ent_loss_list}}\n","\n","\n","\n","\n","\n"],"metadata":{"id":"LlKauWY9XYlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trained_model,loss=train_entity_bert_model(model,train_dataloader,test_dataloader,id_label_l1,id_label_l2,bert_model_name,training_args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34sPtd34bBXN","executionInfo":{"status":"ok","timestamp":1706923941691,"user_tz":300,"elapsed":89340,"user":{"displayName":"srikaran elakurthy","userId":"10476685350926501647"}},"outputId":"500e4ee9-7ec3-4e62-a7a1-b2cff782c5e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/8 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Epoch:  12%|█▎        | 1/8 [00:11<01:17, 11.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0  Train Method Loss: 44.31918716430664\n","Epoch: 0  Val Method Loss: 278.312255859375\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.45054945054945056, 'recall': 0.7454545454545455, 'f1': 0.5616438356164384, 'number': 55}, 'Drug': {'precision': 1.0, 'recall': 0.03333333333333333, 'f1': 0.06451612903225806, 'number': 30}, 'Family': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 20}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 19}, 'MaritalStatus': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 30}, 'Occupation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 29}, 'Tobacco': {'precision': 0.3111111111111111, 'recall': 0.5283018867924528, 'f1': 0.3916083916083916, 'number': 53}, 'overall_precision': 0.38461538461538464, 'overall_recall': 0.2966101694915254, 'overall_f1': 0.33492822966507174, 'overall_accuracy': 0.8732886688027964}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'overall_precision': 0.7142857142857143, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7894736842105262, 'overall_accuracy': 0.9979609670841829}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Epoch:  25%|██▌       | 2/8 [00:22<01:06, 11.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1  Train Method Loss: 23.1181583404541\n","Epoch: 1  Val Method Loss: 155.7523193359375\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.6176470588235294, 'recall': 0.7636363636363637, 'f1': 0.6829268292682927, 'number': 55}, 'Drug': {'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334, 'number': 30}, 'Family': {'precision': 0.2857142857142857, 'recall': 0.3, 'f1': 0.2926829268292683, 'number': 20}, 'LivingSituation': {'precision': 0.20930232558139536, 'recall': 0.47368421052631576, 'f1': 0.2903225806451613, 'number': 19}, 'MaritalStatus': {'precision': 0.3488372093023256, 'recall': 0.5, 'f1': 0.4109589041095891, 'number': 30}, 'Occupation': {'precision': 0.2222222222222222, 'recall': 0.27586206896551724, 'f1': 0.24615384615384614, 'number': 29}, 'Tobacco': {'precision': 0.75, 'recall': 0.9056603773584906, 'f1': 0.8205128205128206, 'number': 53}, 'overall_precision': 0.5016393442622951, 'overall_recall': 0.6483050847457628, 'overall_f1': 0.5656192236598891, 'overall_accuracy': 0.9149431983687737}\n","\n","\n","{'Family': {'precision': 0.6818181818181818, 'recall': 0.8823529411764706, 'f1': 0.7692307692307693, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6521739130434783, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.75, 'overall_accuracy': 0.9973783862510923}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  38%|███▊      | 3/8 [00:32<00:54, 10.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2  Train Method Loss: 13.854171752929688\n","Epoch: 2  Val Method Loss: 140.9980010986328\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.8035714285714286, 'recall': 0.8181818181818182, 'f1': 0.8108108108108109, 'number': 55}, 'Drug': {'precision': 0.7647058823529411, 'recall': 0.8666666666666667, 'f1': 0.8125, 'number': 30}, 'Family': {'precision': 0.39285714285714285, 'recall': 0.55, 'f1': 0.45833333333333337, 'number': 20}, 'LivingSituation': {'precision': 0.27586206896551724, 'recall': 0.42105263157894735, 'f1': 0.3333333333333333, 'number': 19}, 'MaritalStatus': {'precision': 0.5, 'recall': 0.5, 'f1': 0.5, 'number': 30}, 'Occupation': {'precision': 0.35714285714285715, 'recall': 0.3448275862068966, 'f1': 0.3508771929824561, 'number': 29}, 'Tobacco': {'precision': 0.8727272727272727, 'recall': 0.9056603773584906, 'f1': 0.8888888888888888, 'number': 53}, 'overall_precision': 0.6269230769230769, 'overall_recall': 0.690677966101695, 'overall_f1': 0.657258064516129, 'overall_accuracy': 0.9277599766967667}\n","\n","\n","{'Family': {'precision': 0.75, 'recall': 0.8823529411764706, 'f1': 0.8108108108108107, 'number': 17}, 'overall_precision': 0.75, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.8108108108108107, 'overall_accuracy': 0.9982522575007282}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  50%|█████     | 4/8 [00:43<00:43, 10.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3  Train Method Loss: 9.635627746582031\n","Epoch: 3  Val Method Loss: 132.5026397705078\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.7796610169491526, 'recall': 0.8363636363636363, 'f1': 0.8070175438596492, 'number': 55}, 'Drug': {'precision': 0.875, 'recall': 0.9333333333333333, 'f1': 0.9032258064516129, 'number': 30}, 'Family': {'precision': 0.5, 'recall': 0.7, 'f1': 0.5833333333333334, 'number': 20}, 'LivingSituation': {'precision': 0.3793103448275862, 'recall': 0.5789473684210527, 'f1': 0.45833333333333337, 'number': 19}, 'MaritalStatus': {'precision': 0.375, 'recall': 0.5, 'f1': 0.42857142857142855, 'number': 30}, 'Occupation': {'precision': 0.28125, 'recall': 0.3103448275862069, 'f1': 0.2950819672131148, 'number': 29}, 'Tobacco': {'precision': 0.8545454545454545, 'recall': 0.8867924528301887, 'f1': 0.8703703703703703, 'number': 53}, 'overall_precision': 0.6181818181818182, 'overall_recall': 0.7203389830508474, 'overall_f1': 0.665362035225049, 'overall_accuracy': 0.9265948150305855}\n","\n","\n","{'Family': {'precision': 0.75, 'recall': 0.8823529411764706, 'f1': 0.8108108108108107, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.7142857142857143, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7894736842105262, 'overall_accuracy': 0.9979609670841829}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  62%|██████▎   | 5/8 [00:54<00:32, 10.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 4  Train Method Loss: 6.908662796020508\n","Epoch: 4  Val Method Loss: 142.71470642089844\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.711864406779661, 'recall': 0.7636363636363637, 'f1': 0.736842105263158, 'number': 55}, 'Drug': {'precision': 0.8, 'recall': 0.9333333333333333, 'f1': 0.8615384615384616, 'number': 30}, 'Family': {'precision': 0.46153846153846156, 'recall': 0.6, 'f1': 0.5217391304347826, 'number': 20}, 'LivingSituation': {'precision': 0.4074074074074074, 'recall': 0.5789473684210527, 'f1': 0.47826086956521735, 'number': 19}, 'MaritalStatus': {'precision': 0.5625, 'recall': 0.6, 'f1': 0.5806451612903225, 'number': 30}, 'Occupation': {'precision': 0.35135135135135137, 'recall': 0.4482758620689655, 'f1': 0.393939393939394, 'number': 29}, 'Tobacco': {'precision': 0.8070175438596491, 'recall': 0.8679245283018868, 'f1': 0.8363636363636363, 'number': 53}, 'overall_precision': 0.6227106227106227, 'overall_recall': 0.7203389830508474, 'overall_f1': 0.6679764243614932, 'overall_accuracy': 0.9260122341974949}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6818181818181818, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7692307692307693, 'overall_accuracy': 0.9976696766676376}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  75%|███████▌  | 6/8 [01:05<00:21, 10.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 5  Train Method Loss: 5.189341068267822\n","Epoch: 5  Val Method Loss: 141.7391357421875\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.8333333333333334, 'recall': 0.8181818181818182, 'f1': 0.8256880733944955, 'number': 55}, 'Drug': {'precision': 0.8484848484848485, 'recall': 0.9333333333333333, 'f1': 0.888888888888889, 'number': 30}, 'Family': {'precision': 0.48, 'recall': 0.6, 'f1': 0.5333333333333332, 'number': 20}, 'LivingSituation': {'precision': 0.4074074074074074, 'recall': 0.5789473684210527, 'f1': 0.47826086956521735, 'number': 19}, 'MaritalStatus': {'precision': 0.5151515151515151, 'recall': 0.5666666666666667, 'f1': 0.5396825396825397, 'number': 30}, 'Occupation': {'precision': 0.42857142857142855, 'recall': 0.41379310344827586, 'f1': 0.42105263157894735, 'number': 29}, 'Tobacco': {'precision': 0.8867924528301887, 'recall': 0.8867924528301887, 'f1': 0.8867924528301887, 'number': 53}, 'overall_precision': 0.6798418972332015, 'overall_recall': 0.7288135593220338, 'overall_f1': 0.7034764826175869, 'overall_accuracy': 0.9297990096125838}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6818181818181818, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7692307692307693, 'overall_accuracy': 0.9976696766676376}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  88%|████████▊ | 7/8 [01:16<00:10, 10.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 6  Train Method Loss: 4.184082984924316\n","Epoch: 6  Val Method Loss: 144.53701782226562\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.7543859649122807, 'recall': 0.7818181818181819, 'f1': 0.7678571428571429, 'number': 55}, 'Drug': {'precision': 0.8, 'recall': 0.9333333333333333, 'f1': 0.8615384615384616, 'number': 30}, 'Family': {'precision': 0.4666666666666667, 'recall': 0.7, 'f1': 0.56, 'number': 20}, 'LivingSituation': {'precision': 0.46153846153846156, 'recall': 0.631578947368421, 'f1': 0.5333333333333333, 'number': 19}, 'MaritalStatus': {'precision': 0.4594594594594595, 'recall': 0.5666666666666667, 'f1': 0.5074626865671642, 'number': 30}, 'Occupation': {'precision': 0.30303030303030304, 'recall': 0.3448275862068966, 'f1': 0.32258064516129037, 'number': 29}, 'Tobacco': {'precision': 0.8392857142857143, 'recall': 0.8867924528301887, 'f1': 0.8623853211009174, 'number': 53}, 'overall_precision': 0.6240875912408759, 'overall_recall': 0.7245762711864406, 'overall_f1': 0.6705882352941176, 'overall_accuracy': 0.928051267113312}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6818181818181818, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7692307692307693, 'overall_accuracy': 0.9976696766676376}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 8/8 [01:27<00:00, 10.98s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch: 7  Train Method Loss: 3.6720736026763916\n","Epoch: 7  Val Method Loss: 145.64718627929688\n","Method_metrics: \n","\n","{'Alcohol': {'precision': 0.7368421052631579, 'recall': 0.7636363636363637, 'f1': 0.7499999999999999, 'number': 55}, 'Drug': {'precision': 0.8, 'recall': 0.9333333333333333, 'f1': 0.8615384615384616, 'number': 30}, 'Family': {'precision': 0.4827586206896552, 'recall': 0.7, 'f1': 0.5714285714285714, 'number': 20}, 'LivingSituation': {'precision': 0.48, 'recall': 0.631578947368421, 'f1': 0.5454545454545454, 'number': 19}, 'MaritalStatus': {'precision': 0.4722222222222222, 'recall': 0.5666666666666667, 'f1': 0.5151515151515152, 'number': 30}, 'Occupation': {'precision': 0.29411764705882354, 'recall': 0.3448275862068966, 'f1': 0.31746031746031744, 'number': 29}, 'Tobacco': {'precision': 0.8070175438596491, 'recall': 0.8679245283018868, 'f1': 0.8363636363636363, 'number': 53}, 'overall_precision': 0.6190476190476191, 'overall_recall': 0.7161016949152542, 'overall_f1': 0.6640471512770137, 'overall_accuracy': 0.9268861054471308}\n","\n","\n","{'Family': {'precision': 0.7142857142857143, 'recall': 0.8823529411764706, 'f1': 0.7894736842105262, 'number': 17}, 'LivingSituation': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 0}, 'overall_precision': 0.6818181818181818, 'overall_recall': 0.8823529411764706, 'overall_f1': 0.7692307692307693, 'overall_accuracy': 0.9976696766676376}\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}